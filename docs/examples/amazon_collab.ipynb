{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7611af38",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "# AWS Collaboration Benchmark - Interactive Tutorial\n",
        "\n",
        "This notebook demonstrates the AWS Collaboration benchmark using MASEval. The benchmark evaluates multi-agent systems on collaborative task-solving scenarios.\n",
        "\n",
        "## What is this benchmark?\n",
        "\n",
        "The AWS Collaboration benchmark tests how well multi-agent systems can:\n",
        "- **Coordinate multiple specialist agents** (e.g., a travel expert, a restaurant expert)\n",
        "- **Interact with simulated users** across multiple conversation turns\n",
        "- **Use simulated tools** to accomplish realistic tasks\n",
        "- **Meet user-side and system-side goals** (dual evaluation)\n",
        "\n",
        "## Key Metrics\n",
        "\n",
        "- **Overall GSR** (Goal Success Rate): Did the agent satisfy all requirements?\n",
        "- **User GSR**: Did the agent meet user-visible goals?\n",
        "- **System GSR**: Did the agent correctly invoke tools behind the scenes?\n",
        "- **Partial GSR**: What percentage of goals were met?\n",
        "\n",
        "Let's dive in!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3653ea8d",
      "metadata": {},
      "source": [
        "## Overall Structure (Preview)\n",
        "\n",
        "Before we dive into implementation details, let's see the **main execution flow** to understand the big picture. This is what happens when you run the benchmark:\n",
        "\n",
        "```python\n",
        "# Configuration\n",
        "framework = \"smolagents\"  # or \"langgraph\"\n",
        "domain = \"travel\"\n",
        "limit = 5\n",
        "\n",
        "# 1. Download and process raw data\n",
        "process_data(verbose=1)\n",
        "\n",
        "# 2. Load benchmark tasks\n",
        "tasks = load_tasks(domain, limit)\n",
        "\n",
        "# 3. Load agent configuration (defines specialist agents)\n",
        "agent_config = load_agent_config(domain, framework)\n",
        "\n",
        "# 4. Setup result logging\n",
        "output_dir = \"results/\"\n",
        "logger = FileResultLogger(output_dir=output_dir, filename_pattern=f\"{domain}_{framework}_{{timestamp}}.jsonl\")\n",
        "\n",
        "# 5. Create and run the benchmark\n",
        "benchmark = AWSCollabBenchmark(\n",
        "    agent_data=agent_config,\n",
        "    tasks=tasks,\n",
        "    callbacks=[logger]\n",
        ")\n",
        "results = benchmark.run()\n",
        "\n",
        "# 6. Compute and display metrics\n",
        "summary = compute_benchmark_metric(results)\n",
        "print(f\"Success Rate: {summary['success_rate']:.2%}\")\n",
        "```\n",
        "\n",
        "**That's it!** The benchmark handles:\n",
        "- Creating specialist agents (as defined by config)\n",
        "- Simulating user interactions\n",
        "- Simulating tool responses\n",
        "- Evaluating both user-side and system-side goals\n",
        "\n",
        "### Inside the Benchmark: What `AWSCollabBenchmark` Does\n",
        "\n",
        "The `AWSCollabBenchmark` class inherits from MASEval's `Benchmark` base class and implements these key methods:\n",
        "\n",
        "```python\n",
        "class AWSCollabBenchmark(Benchmark):\n",
        "    \n",
        "    def setup_environment(agent_data, task):\n",
        "        # Creates the Environment with simulated tools (FlightBooking, HotelBooking, etc.)\n",
        "        pass\n",
        "    \n",
        "    def setup_user(agent_data, environment, task):\n",
        "        # Creates an LLM-powered User simulator that interacts like a real user\n",
        "        pass\n",
        "    \n",
        "    def setup_agents(agent_data, environment, task, user):\n",
        "        # Creates specialist agents (travel_expert, restaurant_expert) and primary orchestrator\n",
        "        pass\n",
        "    \n",
        "    def setup_evaluators(environment, task, agents, user):\n",
        "        # Creates LLM-based evaluators for user-side and system-side assertions\n",
        "        pass\n",
        "    \n",
        "    def run_agents(agents, task, environment):\n",
        "        # Executes the agent(s) on the task and returns final answer\n",
        "        pass\n",
        "    \n",
        "    def evaluate(evaluators, agents, final_answer, traces):\n",
        "        # Runs both evaluators and computes GSR metrics (user_gsr, system_gsr, overall_gsr)\n",
        "        pass\n",
        "```\n",
        "\n",
        "The base `Benchmark.run()` method orchestrates these steps automatically for each task!\n",
        "\n",
        "Now let's break down each piece step by step."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9421c9e",
      "metadata": {},
      "source": [
        "## Step 1: Imports\n",
        "\n",
        "We start off by importing dependencies. The example here requires severl dependencies beyond the base library dependencies. You can install these:\n",
        "\n",
        "```python\n",
        "pip install maseval[examples] # with pip\n",
        "uv add maseval[examples] # with uv\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e25b7d6e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# All the imports we need for this benchmark\n",
        "import json\n",
        "import os\n",
        "from typing import Any, Dict, List, Optional, Sequence, Tuple\n",
        "\n",
        "# Framework-specific imports (smolagents and langgraph)\n",
        "from google.genai import Client as GoogleGenAIClient\n",
        "from langchain_core.messages import SystemMessage\n",
        "from langchain_core.tools import StructuredTool as LanggraphStructuredTool\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langgraph.graph import END, StateGraph\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "from smolagents import FinalAnswerTool, OpenAIServerModel, Tool as SmolagentsTool, ToolCallingAgent\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "# MASEval core components\n",
        "from maseval import (\n",
        "    AgentAdapter,          # Wraps framework-specific agents\n",
        "    Benchmark,             # Base benchmark class\n",
        "    Environment,           # Simulates tools and external systems\n",
        "    Evaluator,             # Evaluates agent performance\n",
        "    MessageHistory,        # Stores conversation history\n",
        "    ModelAdapter,          # Wraps LLM models\n",
        "    Task,                  # Represents a single benchmark task\n",
        "    TaskCollection,        # Collection of tasks\n",
        "    ToolInvocationHistory, # Tracks tool usage\n",
        "    ToolLLMSimulator,      # LLM-based tool simulator\n",
        "    User,                  # Simulates user behavior\n",
        ")\n",
        "\n",
        "# MASEval utilities\n",
        "from maseval.core.callbacks.result_logger import FileResultLogger\n",
        "from maseval.core.config import ConfigurableMixin\n",
        "from maseval.core.tracing import TraceableMixin\n",
        "\n",
        "# Framework adapters\n",
        "from maseval.interface.agents.langgraph import LangGraphAgentAdapter, LangGraphUser\n",
        "from maseval.interface.agents.smolagents import SmolAgentUser, SmolAgentAdapter\n",
        "from maseval.interface.inference import GoogleGenAIModelAdapter, LiteLLMModelAdapter\n",
        "\n",
        "# Data processing utility\n",
        "from process_data import process_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df7ef796",
      "metadata": {},
      "source": [
        "### Helper Functions: Loading Tasks and Agent Configurations\n",
        "\n",
        "The next two cells define utility functions for loading benchmark data:\n",
        "\n",
        "- **`load_tasks()`**: Loads task objects from the processed JSON files\n",
        "  - Each task becomes a `Task` object with `query`, `environment_data`, `evaluation_data`, and `metadata`\n",
        "  - Returns a `TaskCollection` (an iterable sequence of tasks)\n",
        "  \n",
        "- **`load_agent_config()`**: Loads the recommended multi-agent setup for a domain\n",
        "  - Reads `agents.json` which defines specialist agents and their tools\n",
        "  - Adds framework-specific configuration and model settings\n",
        "  - Returns a dictionary with agent specifications and the primary orchestrator ID\n",
        "\n",
        "These are simple I/O helpersâ€”the interesting logic happens later when we create the benchmark!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a38552d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from typing import Optional, Dict, Any\n",
        "\n",
        "def load_tasks(domain: str, limit: Optional[int] = None) -> TaskCollection:\n",
        "    \"\"\"Load tasks from processed travel data.\"\"\"\n",
        "    if domain not in [\"travel\", \"software\", \"mortgage\"]:\n",
        "        raise ValueError(f\"Unsupported domain: {domain}\")\n",
        "\n",
        "    with open(os.path.join(os.getcwd(), \"data\", domain, \"tasks.json\"), \"r\") as f:\n",
        "        tasks_list = json.load(f)\n",
        "\n",
        "    tasks_data = []\n",
        "    for task_dict in tasks_list[:limit] if limit else tasks_list:\n",
        "        metadata = task_dict.get(\"metadata\", {})\n",
        "        tasks_data.append(\n",
        "            Task(\n",
        "                query=task_dict[\"query\"],\n",
        "                environment_data=task_dict[\"environment_data\"],\n",
        "                evaluation_data=task_dict[\"evaluation_data\"],\n",
        "                metadata=metadata,\n",
        "            )\n",
        "        )\n",
        "    return TaskCollection(tasks_data)\n",
        "\n",
        "def load_agent_config(domain: str, framework: str) -> Dict[str, Any]:\n",
        "    \"\"\"Load agent configuration from processed travel data.\"\"\"\n",
        "\n",
        "    # loads the recommended setup of agents for the domain as per AWS Collab benchmark\n",
        "    with open(os.path.join(os.getcwd(), \"data\", domain, \"agents.json\"), \"r\") as f:\n",
        "        agents_data = json.load(f)\n",
        "\n",
        "    # adds more config to the agent data\n",
        "    agents_data[\"framework\"] = framework  # Set framework as specified\n",
        "    for agent in agents_data.get(\"agents\", []):\n",
        "        agent[\"model_config\"] = {\"model_id\": \"gemini-2.5-flash\", \"temperature\": 0.7}\n",
        "\n",
        "    return agents_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bed2a1a5",
      "metadata": {},
      "source": [
        "### Helper Function: Model Initialization\n",
        "\n",
        "The `get_model()` function creates a `ModelAdapter` instance for a given LLM.\n",
        "\n",
        "**Why ModelAdapter?** MASEval uses `ModelAdapter` as a unified interface for different LLM providers:\n",
        "- Wraps provider-specific clients (Google GenAI, OpenAI, etc.)\n",
        "- Provides a consistent `.generate()` method across all models\n",
        "- Makes it easy to swap models without changing benchmark code\n",
        "\n",
        "This model will be used for:\n",
        "- **Tool simulation**: Generating realistic tool responses\n",
        "- **User simulation**: Simulating multi-turn user conversations\n",
        "- **Evaluation**: LLM-as-a-judge for assertion checking\n",
        "\n",
        "In this notebook, we default to `gemini-2.5-flash` for speed, but you can use other models like `gemini-2.5-pro` or `gpt-4o`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa309733",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_model(model_id: str = \"gemini-2.5-flash\", **kwargs) -> ModelAdapter:\n",
        "    if model_id == \"gemini-2.5-flash\":\n",
        "        google_client = GoogleGenAIClient(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
        "        bare_model = GoogleGenAIModelAdapter(\n",
        "            google_client, model_id=\"gemini-2.5-flash\", default_generation_params=kwargs\n",
        "        )\n",
        "    elif model_id == \"gemini-2.5-pro\":\n",
        "        google_client = GoogleGenAIClient(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
        "        bare_model = GoogleGenAIModelAdapter(\n",
        "            google_client, model_id=\"gemini-2.5-pro\", default_generation_params=kwargs\n",
        "        )\n",
        "    elif model_id == \"gpt-4o\":\n",
        "        bare_model = LiteLLMModelAdapter(model_id=\"gpt-4o\", default_generation_params=kwargs)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported model_id: {model_id}\")\n",
        "    return bare_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33b05873",
      "metadata": {},
      "source": [
        "## Step 2: Load Data & Configure\n",
        "\n",
        "Before we can run the benchmark, we need to:\n",
        "\n",
        "1. **Download and process the raw benchmark data** from the AWS Collaboration benchmark repository\n",
        "2. **Load tasks** for our chosen domain (travel, software, or mortgage)\n",
        "3. **Configure which agent framework** to use (smolagents or langgraph)\n",
        "\n",
        "### Configuration Parameters\n",
        "\n",
        "Let's start by setting three key configuration variables:\n",
        "\n",
        "- **`framework`**: Which agent framework to use for orchestrating agents\n",
        "  - `\"smolagents\"`: HuggingFace's lightweight agent framework with built-in multi-agent support\n",
        "  - `\"langgraph\"`: LangChain's stateful graph-based agent framework\n",
        "  \n",
        "- **`domain`**: Which benchmark domain to evaluate on\n",
        "  - `\"travel\"`: Travel planning scenarios (flights, hotels, restaurants)\n",
        "  - `\"software\"`: Software development scenarios (bug tracking, code review)\n",
        "  - `\"mortgage\"`: Mortgage application scenarios (loan processing, document verification)\n",
        "  \n",
        "- **`limit`**: How many tasks to run from the domain\n",
        "  - Set this to a small number (e.g., 3-5) for quick testing\n",
        "  - Use `None` to run all tasks (~30 per domain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08b22057",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration: Set these to customize your benchmark run\n",
        "framework = \"smolagents\"  # Agent framework: \"smolagents\" or \"langgraph\"\n",
        "domain = \"travel\"         # Benchmark domain: \"travel\", \"software\", or \"mortgage\"\n",
        "limit = 3                 # Number of tasks to run (use None for all ~30 tasks)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73c527eb",
      "metadata": {},
      "source": [
        "### Download and Process Raw Data\n",
        "\n",
        "The AWS Collaboration benchmark data is hosted on GitHub. The `process_data()` function:\n",
        "\n",
        "1. **Downloads** raw JSON files from the official AWS benchmark repository\n",
        "2. **Processes** the data into MASEval's `Task` format with three key components:\n",
        "   - **`environment_data`**: Available tools and their specifications\n",
        "   - **`evaluation_data`**: Assertions to check if the agent succeeded\n",
        "   - **`metadata`**: Scenario descriptions and context\n",
        "3. **Saves** processed data to `data/<domain>/` for reuse\n",
        "\n",
        "This only needs to run once (subsequent runs will skip download if data exists)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "defa223e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download and process raw benchmark data from AWS repository\n",
        "# This creates a data/<domain>/ directory with tasks.json, agents.json, and prompt templates\n",
        "process_data(verbose=1)\n",
        "\n",
        "# Load tasks for the chosen domain\n",
        "# Returns a TaskCollection - an iterable of Task objects with query, environment_data, and evaluation_data\n",
        "tasks = load_tasks(domain=domain, limit=limit)\n",
        "\n",
        "print(f\"Successfully loaded {len(tasks)} tasks from the '{domain}' domain\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8797aa89",
      "metadata": {},
      "source": [
        "### Inspect a Sample Task\n",
        "\n",
        "Let's examine what a task looks like. Each `Task` object contains:\n",
        "\n",
        "- **`query`**: The initial user request that starts the conversation\n",
        "- **`id`**: A unique identifier (auto-generated UUID)\n",
        "- **`environment_data`**: Data needed to set up the environment (tools, user profile, etc.)\n",
        "- **`user_data`**: Data specific to user simulation configuration\n",
        "- **`evaluation_data`**: Data needed to evaluate agent performance (assertions)\n",
        "- **`metadata`**: Any additional metadata (scenario descriptions, context, etc.)\n",
        "\n",
        "**Evaluation Assertions** define success criteria with two types:\n",
        "- **User-side assertions** (prefixed with `user:`): Check if the user's goals were met\n",
        "- **System-side assertions** (prefixed with `agent:`): Check if correct tools were used with correct parameters\n",
        "\n",
        "This dual evaluation approach (user + system) is a key feature of the AWS Collaboration benchmark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d540098c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examine the first task in detail\n",
        "sample_task = tasks[0]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"TASK QUERY (Initial User Request):\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"{sample_task.query}\\n\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"SCENARIO (Full Context - truncated):\")\n",
        "print(\"=\" * 80)\n",
        "scenario = sample_task.metadata.get('scenario', 'N/A')\n",
        "print(f\"{scenario[:300]}...\\n\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"EVALUATION ASSERTIONS:\")\n",
        "print(\"=\" * 80)\n",
        "assertions = sample_task.evaluation_data.get(\"assertions\", [])\n",
        "for i, assertion in enumerate(assertions, 1):\n",
        "    # Show which type of assertion it is\n",
        "    assertion_type = \"USER-SIDE\" if assertion.lower().startswith(\"user:\") else \"SYSTEM-SIDE\"\n",
        "    print(f\"{i}. [{assertion_type}] {assertion}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66786acb",
      "metadata": {},
      "source": [
        "### Load Agent Configuration\n",
        "\n",
        "Each domain comes with a recommended multi-agent setup from the AWS benchmark:\n",
        "\n",
        "- **Specialist agents**: Domain experts with access to specific tools (e.g., `travel_expert` can book flights/hotels)\n",
        "- **Primary orchestrator agent**: Coordinates specialist agents and handles user interaction\n",
        "\n",
        "The configuration specifies:\n",
        "- Which agents exist and their roles\n",
        "- Which tools each agent can use\n",
        "- System prompts defining agent behavior\n",
        "- Which agent is the primary orchestrator (`primary_agent_id`)\n",
        "\n",
        "MASEval supports both **smolagents** (using `managed_agents`) and **langgraph** (using tool-calling + routing) to implement this multi-agent pattern."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa34d24f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the recommended agent configuration for the chosen domain\n",
        "agent_config = load_agent_config(domain=domain, framework=framework)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"AGENT CONFIGURATION OVERVIEW:\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Framework: {agent_config['framework']}\")\n",
        "print(f\"Primary Orchestrator: {agent_config['primary_agent_id']}\")\n",
        "print(f\"Total Agents: {len(agent_config['agents'])}\\n\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"AGENT DETAILS:\")\n",
        "print(\"=\" * 80)\n",
        "for agent in agent_config[\"agents\"]:\n",
        "    print(f\"\\n{agent['agent_id']} - {agent['agent_name']}\")\n",
        "    print(f\"   Tools: {', '.join(agent.get('tools', [])) or 'None (orchestrator role only)'}\")\n",
        "    print(f\"   Role: {agent['agent_instruction'][:150]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15a24564",
      "metadata": {},
      "source": [
        "### Setup Result Logger\n",
        "\n",
        "Before running the benchmark, we'll configure a callback to save results to disk.\n",
        "\n",
        "**What is a callback?** In MASEval, callbacks are hooks that get notified during benchmark execution. The `FileResultLogger` callback:\n",
        "\n",
        "- Automatically saves each task's results to a JSONL file (one JSON object per line)\n",
        "- Captures metrics, traces, and evaluation reports\n",
        "- Uses timestamped filenames to avoid overwriting previous runs\n",
        "- Enables later analysis without re-running expensive evaluations\n",
        "\n",
        "The results will be saved to `results/<domain>_<framework>_<timestamp>.jsonl`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c23b86c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup result logging to save benchmark results\n",
        "output_dir = os.path.join(os.getcwd(), \"results\")\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Create a FileResultLogger callback with timestamped filename\n",
        "logger = FileResultLogger(\n",
        "    output_dir=output_dir,\n",
        "    filename_pattern=f\"{domain}_{framework}_{{timestamp}}.jsonl\"  # e.g., travel_smolagents_20231111_143025.jsonl\n",
        ")\n",
        "\n",
        "print(f\"Results will be saved to: {output_dir}/\")\n",
        "print(f\"  Filename pattern: {domain}_{framework}_<timestamp>.jsonl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d94ed727",
      "metadata": {},
      "source": [
        "## Step 3: Environment Setup - Tools and User Simulation\n",
        "\n",
        "Now we'll implement the core simulation components:\n",
        "\n",
        "1. **`GenericTool`**: Framework-agnostic tool with LLM-based response simulation\n",
        "2. **Framework-specific wrappers**: `GenericSmolagentsTool` and `GenericLanggraphTool` \n",
        "3. **`TravelEnvironment`**: Environment class that creates and manages tools\n",
        "4. **User simulation**: LLM-powered user that responds realistically to agent queries\n",
        "\n",
        "These components enable **realistic multi-turn interactions** without needing real APIs!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d1d4417",
      "metadata": {},
      "source": [
        "### Inspect Available Tools in Task\n",
        "\n",
        "Before implementing the environment, let's see what tools the task provides:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "037a4af4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examine tools available in this task's environment\n",
        "env_data = sample_task.environment_data\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"AVAILABLE TOOLS IN ENVIRONMENT:\")\n",
        "print(\"=\" * 80)\n",
        "for tool in env_data.get(\"tools\", []):\n",
        "    print(f\"\\nðŸ”§ {tool['tool_name']}\")\n",
        "    print(f\"   Description: {tool.get('description', 'N/A')}\")\n",
        "    print(f\"   Actions:\")\n",
        "    for action in tool.get(\"actions\", []):\n",
        "        action_desc = action.get('description', 'N/A')[:100]\n",
        "        print(f\"      â€¢ {action['name']}: {action_desc}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "827bce7d",
      "metadata": {},
      "source": [
        "### Implement GenericTool (Framework-Agnostic)\n",
        "\n",
        "The `GenericTool` class provides the core tool logic that works with any agent framework:\n",
        "\n",
        "**Key Features:**\n",
        "- **LLM-based simulation**: Uses `ToolLLMSimulator` to generate realistic responses\n",
        "- **Invocation tracking**: Records all tool calls in a `ToolInvocationHistory`\n",
        "- **Schema conversion**: Converts JSON schemas to tool input specifications\n",
        "- **Tracing & Configuration**: Implements mixins for gathering execution traces and config\n",
        "\n",
        "**How it works:**\n",
        "1. Agent calls tool with parameters (e.g., `search_flights(origin=\"NYC\", destination=\"LAX\")`)\n",
        "2. `GenericTool.__call__()` forwards to `ToolLLMSimulator`\n",
        "3. Simulator uses an LLM to generate a realistic response based on tool description\n",
        "4. Response is logged in history and returned to agent\n",
        "\n",
        "This approach lets us benchmark agents **without real APIs or mock data**â€”the LLM generates contextually appropriate responses!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ba36399",
      "metadata": {},
      "outputs": [],
      "source": [
        "class GenericTool(TraceableMixin, ConfigurableMixin):\n",
        "    \"\"\"Framework-agnostic tool with execution logic and tracing.\"\"\"\n",
        "\n",
        "    def __init__(self, spec: Dict[str, Any]):\n",
        "        super().__init__()\n",
        "        self.name = spec[\"name\"]\n",
        "        self.description = spec.get(\"description\", \"\")\n",
        "        self.input_schema = spec.get(\"input_schema\", {})\n",
        "        self.output_schema = \"string\"\n",
        "        self.history = ToolInvocationHistory()\n",
        "\n",
        "        # Create schema to inputs mapping for ToolLLMSimulator\n",
        "        self.inputs = self._schema_to_inputs(self.input_schema)\n",
        "\n",
        "        # Create LLM-based simulator\n",
        "        self.simulator = ToolLLMSimulator(\n",
        "            model=get_model(),\n",
        "            template=None,  # Uses default template\n",
        "            tool_name=self.name,\n",
        "            tool_description=self.description,\n",
        "            tool_inputs=self.inputs,\n",
        "            max_try=1,\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def _schema_to_inputs(input_schema: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Convert JSON schema to inputs format for Tool interface.\"\"\"\n",
        "        inputs_dict = {}\n",
        "        for k, prop in input_schema.get(\"properties\", {}).items():\n",
        "            declared = prop.get(\"data_type\") or prop.get(\"type\")\n",
        "            declared_type = (\n",
        "                [d if isinstance(d, str) else \"Any\" for d in declared]\n",
        "                if isinstance(declared, list)\n",
        "                else (declared if isinstance(declared, str) else \"Any\")\n",
        "            )\n",
        "            inputs_dict[k] = {\"type\": declared_type, \"description\": prop.get(\"description\", \"\")}\n",
        "        return inputs_dict\n",
        "\n",
        "    def __call__(self, **kwargs) -> str:\n",
        "        \"\"\"Execute the tool with given inputs.\n",
        "\n",
        "        Uses LLM simulator to generate realistic response.\n",
        "        \"\"\"\n",
        "        response_text, details = self.simulator(actual_inputs=kwargs)\n",
        "        self.history.add_invocation(\n",
        "            inputs={\"kwargs\": kwargs},\n",
        "            outputs=response_text,\n",
        "            status=\"<Unknown>\",\n",
        "            timestamp=None,\n",
        "            meta=details,\n",
        "        )\n",
        "        return response_text\n",
        "    \n",
        "    def gather_traces(self) -> Dict[str, Any]:\n",
        "        \"\"\"Gather execution traces from this tool.\"\"\"\n",
        "        return {\n",
        "            **super().gather_traces(),\n",
        "            \"name\": self.name,\n",
        "            \"invocations\": self.history.to_list(),\n",
        "            \"total_invocations\": len(self.history.to_list()),\n",
        "        }\n",
        "    \n",
        "    def gather_config(self) -> Dict[str, Any]:\n",
        "        \"\"\"Gather configuration from this tool.\"\"\"\n",
        "        return {\n",
        "            **super().gather_config(),\n",
        "            \"name\": self.name,\n",
        "            \"description\": self.description,\n",
        "            \"input_schema\": self.input_schema,\n",
        "        }\n",
        "\n",
        "    def __repr__(self):\n",
        "        arguments = \", \".join([f\"{k}: {v['type']}\" for k, v in self.inputs.items()]) if isinstance(self.inputs, dict) else str(self.inputs)\n",
        "        return f\"{self.__class__.__name__}: {self.name}({arguments}) -> {str(self.output_schema)}\"\n",
        "\n",
        "print(\"GenericTool class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2a586c4",
      "metadata": {},
      "source": [
        "### Implement Framework-Specific Tool Wrappers\n",
        "\n",
        "Since each agent framework has its own tool interface expectations, we need thin wrapper classes:\n",
        "\n",
        "**GenericSmolagentsTool (for smolagents):**\n",
        "- Inherits from `smolagents.Tool`\n",
        "- Implements `forward()` method (required by smolagents)\n",
        "- Wraps `GenericTool` via composition (NOT inheritance to avoid `__call__` conflicts)\n",
        "- Delegates execution to wrapped `GenericTool`\n",
        "\n",
        "**GenericLanggraphTool (for langgraph):**\n",
        "- Wraps `GenericTool` using LangChain's `StructuredTool`\n",
        "- Converts to LangChain's tool format\n",
        "- Delegates execution to wrapped `GenericTool`\n",
        "\n",
        "**Why composition?** Different frameworks expect different `__call__` signatures. By wrapping `GenericTool` instead of inheriting, we avoid method conflicts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "addeed2c",
      "metadata": {},
      "outputs": [],
      "source": [
        "class GenericSmolagentsTool(SmolagentsTool, ConfigurableMixin, TraceableMixin):\n",
        "    \"\"\"Smolagents-specific wrapper around GenericTool.\n",
        "    \n",
        "    Uses composition (NOT inheritance from GenericTool) to avoid __call__ conflicts.\n",
        "    Smolagents expects: __call__ â†’ forward() â†’ actual work\n",
        "    \"\"\"\n",
        "    \n",
        "    # Skip smolagents signature validation (like other smolagents tool wrappers)\n",
        "    skip_forward_signature_validation = True\n",
        "    \n",
        "    def __init__(self, generic_tool: GenericTool):\n",
        "        # Store wrapped GenericTool instance\n",
        "        self.generic_tool = generic_tool\n",
        "        \n",
        "        # Set smolagents-required attributes from wrapped tool\n",
        "        self.name = self.generic_tool.name\n",
        "        self.description = self.generic_tool.description\n",
        "        self.inputs = self.generic_tool.inputs\n",
        "        self.output_type = self.generic_tool.output_schema\n",
        "        \n",
        "        # Initialize smolagents.Tool\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, **kwargs) -> str:\n",
        "        \"\"\"Smolagents forward method - delegates to wrapped GenericTool.\"\"\"\n",
        "        return self.generic_tool(**kwargs)\n",
        "    \n",
        "    def gather_traces(self) -> Dict[str, Any]:\n",
        "        \"\"\"Gather traces from wrapped tool.\"\"\"\n",
        "        return self.generic_tool.gather_traces()\n",
        "    \n",
        "    def gather_config(self) -> Dict[str, Any]:\n",
        "        \"\"\"Gather config from wrapped tool.\"\"\"\n",
        "        return self.generic_tool.gather_config()\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return f\"GenericSmolagentsTool({self.generic_tool})\"\n",
        "\n",
        "\n",
        "class GenericLanggraphTool(ConfigurableMixin, TraceableMixin):\n",
        "    \"\"\"LangGraph-specific tool wrapper.\"\"\"\n",
        "    \n",
        "    def __init__(self, generic_tool: GenericTool):\n",
        "        self.generic_tool = generic_tool\n",
        "        \n",
        "        # Expose name for Environment's _tools_dict storage\n",
        "        self.name = generic_tool.name\n",
        "        \n",
        "        # Create LangChain StructuredTool wrapper\n",
        "        self.tool = LanggraphStructuredTool.from_function(\n",
        "            func=generic_tool.__call__,\n",
        "            name=generic_tool.name,\n",
        "            description=generic_tool.description,\n",
        "        )\n",
        "    \n",
        "    def __call__(self, *args, **kwargs):\n",
        "        \"\"\"Delegate to wrapped LangChain tool.\"\"\"\n",
        "        return self.tool(*args, **kwargs)\n",
        "    \n",
        "    def gather_traces(self) -> Dict[str, Any]:\n",
        "        \"\"\"Gather traces from wrapped tool.\"\"\"\n",
        "        return self.generic_tool.gather_traces()\n",
        "    \n",
        "    def gather_config(self) -> Dict[str, Any]:\n",
        "        \"\"\"Gather config from wrapped tool.\"\"\"\n",
        "        return self.generic_tool.gather_config()\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return f\"GenericLanggraphTool({self.generic_tool})\"\n",
        "\n",
        "print(\"Framework-specific tool wrappers defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef0a5c32",
      "metadata": {},
      "source": [
        "### Implement TravelEnvironment\n",
        "\n",
        "The `TravelEnvironment` class extends MASEval's `Environment` base class to create and manage tools:\n",
        "\n",
        "**Key Responsibilities:**\n",
        "1. **`setup_state()`**: Initialize environment state from task data\n",
        "2. **`create_tools()`**: Create tool instances from tool specifications in task\n",
        "   - Iterates through tool specs and actions\n",
        "   - Creates `GenericTool` for each action\n",
        "   - Wraps in framework-specific wrapper (`GenericSmolagentsTool` or `GenericLanggraphTool`)\n",
        "   - Registers tools with benchmark for tracing\n",
        "3. **`get_tools_for_agent()`**: Filter tools by name for specific agents\n",
        "   - Specialist agents only get their assigned tools\n",
        "   - Orchestrator agents typically get no tools (they delegate to specialists)\n",
        "\n",
        "The base `Environment` class automatically stores tools in `_tools_dict` for tracing and provides the `get_tools()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96206883",
      "metadata": {},
      "outputs": [],
      "source": [
        "class TravelEnvironment(Environment):\n",
        "    \"\"\"Environment that creates framework-agnostic tools and manages them.\n",
        "    \n",
        "    This class demonstrates the pattern:\n",
        "    1. Create GenericTool (framework-agnostic)\n",
        "    2. Wrap in framework-specific wrapper\n",
        "    3. Register with benchmark for tracing\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, task_data: Dict[str, Any], framework: str, benchmark: Optional[Benchmark] = None):\n",
        "        self.benchmark = benchmark\n",
        "        self._framework = framework\n",
        "        super().__init__(task_data)\n",
        "\n",
        "    def setup_state(self, task_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Initialize environment state from task data.\"\"\"\n",
        "        return task_data.get(\"environment_data\", {})\n",
        "\n",
        "    def create_tools(self) -> list:\n",
        "        \"\"\"Create tool instances from tool specifications.\n",
        "        \n",
        "        Returns:\n",
        "            List of tool objects. The base Environment class will handle\n",
        "            storing them in self._tools_dict for tracing.\n",
        "        \"\"\"\n",
        "        tools_list = []\n",
        "        seen_tool_names = set()\n",
        "        \n",
        "        # Determine which wrapper class to use based on framework\n",
        "        framework = self._framework\n",
        "        tool_classes = {\"smolagents\": GenericSmolagentsTool, \"langgraph\": GenericLanggraphTool}\n",
        "        if framework not in tool_classes:\n",
        "            raise ValueError(f\"Unsupported framework: {framework}. Must be one of {list(tool_classes.keys())}\")\n",
        "        WrapperClass = tool_classes[framework]\n",
        "\n",
        "        for tool_spec in self.state.get(\"tools\", []):\n",
        "            for action_spec in tool_spec.get(\"actions\", []):\n",
        "                tool_name = action_spec.get(\"name\")\n",
        "                if tool_name and tool_name not in seen_tool_names:\n",
        "                    # Step 1: Create framework-agnostic GenericTool\n",
        "                    generic_tool = GenericTool(action_spec)\n",
        "                    \n",
        "                    # Step 2: Wrap it for the specific framework\n",
        "                    tool = WrapperClass(generic_tool)\n",
        "                    tools_list.append(tool)\n",
        "                    seen_tool_names.add(tool_name)\n",
        "                    \n",
        "                    # Step 3: Register tool with benchmark for top-level trace/config collection\n",
        "                    if self.benchmark:\n",
        "                        self.benchmark.register(\"tools\", tool_name, tool)\n",
        "\n",
        "        return tools_list\n",
        "\n",
        "    def get_tools_for_agent(self, agent_tools_names: List[str]) -> List[Any]:\n",
        "        \"\"\"Get framework-specific tools for an agent by tool name.\n",
        "        \n",
        "        This is a custom method for this benchmark that filters tools by name.\n",
        "        Tools are already the correct type from create_tools().\n",
        "        \"\"\"\n",
        "        agent_tools = []\n",
        "        for tool_name in agent_tools_names:\n",
        "            for tool_spec in self.state.get(\"tools\", []):\n",
        "                if tool_spec.get(\"tool_name\") == tool_name:\n",
        "                    for action_spec in tool_spec.get(\"actions\", []):\n",
        "                        action_name = action_spec.get(\"name\")\n",
        "                        # Look up tool in the _tools_dict managed by base Environment\n",
        "                        if action_name in self._tools_dict:\n",
        "                            agent_tools.append(self._tools_dict[action_name])\n",
        "        return agent_tools\n",
        "\n",
        "print(\"TravelEnvironment class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c888777",
      "metadata": {},
      "source": [
        "## Step 4: Implement Evaluator\n",
        "The evaluator is crucial for the AWS Collaboration benchmark's **dual evaluation** approach. We'll implement:\n",
        "\n",
        "1. **`LLMAssertionEvaluator`**: LLM-based evaluator that checks assertions\n",
        "2. **Dual evaluation**: Separate user-side and system-side evaluation\n",
        "3. **GSR metrics**: Goal Success Rate computation\n",
        "\n",
        "**Why LLM-as-a-Judge?** \n",
        "- Assertions are natural language (e.g., \"User was offered a flight departing in the afternoon\")\n",
        "- Requires understanding context and nuance\n",
        "- LLMs can judge better than rule-based matching"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26e63372",
      "metadata": {},
      "source": [
        "### Implement LLMAssertionEvaluator\n",
        "\n",
        "The `LLMAssertionEvaluator` class extends MASEval's `Evaluator` base class:\n",
        "\n",
        "**Key Responsibilities:**\n",
        "1. **Parse assertions**: Filter assertions by type (user-side vs system-side)\n",
        "   - User assertions start with `user:` or have no prefix\n",
        "   - System assertions start with `agent:`\n",
        "   \n",
        "2. **Format evaluation prompt**: Build a prompt with:\n",
        "   - Scenario description (what user wants)\n",
        "   - Conversation history (agent-user interaction)\n",
        "   - Tool invocations (for system-side only)\n",
        "   - Assertions to check\n",
        "   \n",
        "3. **LLM judgment**: Use LLM to evaluate each assertion as True/False with reasoning\n",
        "\n",
        "4. **Compute GSR metrics**:\n",
        "   - **GSR** (Goal Success Rate): 1.0 if ALL assertions true, else 0.0\n",
        "   - **Partial GSR**: Percentage of assertions that are true\n",
        "\n",
        "**Evaluation Types:**\n",
        "- **User-side (`gsr_type=\"user\"`)**: Did the agent satisfy user-visible goals?\n",
        "- **System-side (`gsr_type=\"system\"`)**: Did the agent use tools correctly?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "957809d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "class LLMAssertionEvaluator(Evaluator):\n",
        "    \"\"\"Framework-agnostic evaluator using LLM for assertion checking.\n",
        "    \n",
        "    This evaluator uses an LLM to judge whether assertions are satisfied\n",
        "    by examining the conversation history and tool invocations.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: ModelAdapter,\n",
        "        task: Task,\n",
        "        gsr_type: str = \"user\",\n",
        "    ):\n",
        "        \"\"\"Initialize the evaluator.\n",
        "        \n",
        "        Args:\n",
        "            model: The model to use for evaluation (LLM-as-a-judge)\n",
        "            task: The task being evaluated (contains assertions and scenario)\n",
        "            gsr_type: Either \"user\" or \"system\" for evaluation type\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.task = task\n",
        "        self.gsr_type = gsr_type\n",
        "        \n",
        "        # Load appropriate prompt template\n",
        "        # These templates guide the LLM on how to evaluate assertions\n",
        "        template_file = \"user.txt\" if gsr_type == \"user\" else \"system.txt\"\n",
        "        template_path = os.path.join(os.getcwd(), \"prompt_templates\", template_file)\n",
        "        with open(template_path, \"r\") as f:\n",
        "            self.template = f.read()\n",
        "\n",
        "    def __call__(self, trace: MessageHistory, tool_traces: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
        "        \"\"\"Evaluate the trace against assertions.\n",
        "        \n",
        "        Args:\n",
        "            trace: Message history from agent execution (agent-user conversation)\n",
        "            tool_traces: Optional tool traces from environment (required for system-side evaluation)\n",
        "        \n",
        "        Returns:\n",
        "            Dict with keys: gsr, partial_gsr, report (list of assertion judgments)\n",
        "        \"\"\"\n",
        "        # Parse assertions for this evaluation type (filter user vs system)\n",
        "        all_assertions = self.task.evaluation_data[\"assertions\"]\n",
        "        assertions = self._parse_assertions(all_assertions)\n",
        "        \n",
        "        # Format conversation history into a readable string\n",
        "        history = self._format_conversation_history(trace)\n",
        "        \n",
        "        # Get scenario description (what the user wants to accomplish)\n",
        "        scenario = self.task.metadata.get(\"scenario\", None)\n",
        "        if scenario is None:\n",
        "            raise ValueError(\"Task metadata must include 'scenario' for GSR evaluation\")\n",
        "        \n",
        "        # Build evaluation prompt based on type\n",
        "        if self.gsr_type == \"user\":\n",
        "            # User-side: Only needs conversation history\n",
        "            prompt = self.template.replace(\"{{scenario}}\", scenario)\n",
        "            prompt = prompt.replace(\"{{history}}\", history)\n",
        "            prompt = prompt.replace(\"{{assertions}}\", \"\\n\".join(assertions))\n",
        "        else:  # system\n",
        "            # System-side: Also needs tool invocations\n",
        "            invocations = self._format_tool_invocations(tool_traces or {})\n",
        "            prompt = self.template.replace(\"{{scenario}}\", scenario)\n",
        "            prompt = prompt.replace(\"{{history}}\", history)\n",
        "            prompt = prompt.replace(\"{{invocations}}\", invocations)\n",
        "            prompt = prompt.replace(\"{{assertions}}\", \"\\n\".join(assertions))\n",
        "        \n",
        "        # Get LLM judgment\n",
        "        response = self.model.generate(prompt).strip()\n",
        "        \n",
        "        # Clean up response (remove markdown code blocks if present)\n",
        "        response = response.strip(\"```\").strip(\"json\").strip()\n",
        "        \n",
        "        try:\n",
        "            # Parse JSON response\n",
        "            report = json.loads(response)\n",
        "            \n",
        "            # Handle wrapped responses (e.g., {\"assertions\": [...]})\n",
        "            for wrapper_key in [\"assertions\", \"results\"]:\n",
        "                if isinstance(report, dict) and wrapper_key in report:\n",
        "                    report = report[wrapper_key]\n",
        "                    break\n",
        "            \n",
        "            # Ensure it's a list\n",
        "            if isinstance(report, dict):\n",
        "                report = [report]\n",
        "            \n",
        "            # Compute GSR metrics from the report\n",
        "            gsr, partial_gsr = self._compute_gsr(report)\n",
        "            \n",
        "            # Add assertion type to each result for tracking\n",
        "            for item in report:\n",
        "                item[\"assertion_type\"] = self.gsr_type\n",
        "            \n",
        "            return {\n",
        "                \"gsr\": gsr,\n",
        "                \"partial_gsr\": partial_gsr,\n",
        "                \"report\": report\n",
        "            }\n",
        "            \n",
        "        except json.JSONDecodeError as e:\n",
        "            # Handle parse errors gracefully\n",
        "            return {\n",
        "                \"gsr\": 0.0,\n",
        "                \"partial_gsr\": 0.0,\n",
        "                \"report\": [],\n",
        "                \"error\": f\"Failed to decode JSON: {e}\",\n",
        "                \"raw_response\": response\n",
        "            }\n",
        "    \n",
        "    def _parse_assertions(self, assertions: List[str]) -> List[str]:\n",
        "        \"\"\"Parse assertions and filter by type (user or system).\n",
        "        \n",
        "        User assertions: start with \"user:\" or have no prefix\n",
        "        System assertions: start with \"agent:\"\n",
        "        \"\"\"\n",
        "        parsed = []\n",
        "        user_prefix, system_prefix = \"user:\", \"agent:\"\n",
        "        \n",
        "        for assertion in assertions:\n",
        "            assertion = assertion.strip()\n",
        "            \n",
        "            if self.gsr_type == \"user\":\n",
        "                if assertion.lower().startswith(user_prefix):\n",
        "                    # Remove prefix\n",
        "                    parsed.append(assertion[len(user_prefix):].strip())\n",
        "                elif not assertion.lower().startswith(system_prefix):\n",
        "                    # No prefix means user assertion (AWS default)\n",
        "                    parsed.append(assertion)\n",
        "            else:  # system\n",
        "                if assertion.lower().startswith(system_prefix):\n",
        "                    # Remove prefix\n",
        "                    parsed.append(assertion[len(system_prefix):].strip())\n",
        "        \n",
        "        return parsed\n",
        "    \n",
        "    def _format_conversation_history(self, trace: MessageHistory) -> str:\n",
        "        \"\"\"Format conversation history for the prompt.\n",
        "        \n",
        "        Converts MessageHistory into a readable string format.\n",
        "        \"\"\"\n",
        "        formatted_lines = []\n",
        "        for msg in trace:\n",
        "            role = msg.get(\"role\", \"unknown\")\n",
        "            content = msg.get(\"content\", \"\")\n",
        "            \n",
        "            # Handle content that might be a list (smolagents format)\n",
        "            if isinstance(content, list):\n",
        "                content_str = \" \".join(\n",
        "                    item.get(\"text\", \"\") if isinstance(item, dict) else str(item)\n",
        "                    for item in content\n",
        "                )\n",
        "            else:\n",
        "                content_str = str(content)\n",
        "            \n",
        "            formatted_lines.append(f\"{role}: {content_str}\")\n",
        "        \n",
        "        return \"\\n\".join(formatted_lines)\n",
        "    \n",
        "    def _format_tool_invocations(self, tool_traces: Dict[str, Any]) -> str:\n",
        "        \"\"\"Format tool invocations from environment traces for system-side evaluation.\n",
        "        \n",
        "        Args:\n",
        "            tool_traces: Tool traces dictionary from execution_traces[\"environment\"][\"tools\"]\n",
        "        \n",
        "        Returns:\n",
        "            Formatted string showing which tools were called with what inputs/outputs\n",
        "        \"\"\"\n",
        "        invocations_lines = []\n",
        "        \n",
        "        for tool_name, tool_data in tool_traces.items():\n",
        "            # Check if tool has invocation records\n",
        "            invocations = tool_data.get(\"invocations\", [])\n",
        "            if invocations:\n",
        "                for inv in invocations:\n",
        "                    invocations_lines.append(\n",
        "                        f\"Tool: {tool_name}\\n\"\n",
        "                        f\"  Inputs: {inv.get('inputs', {})}\\n\"\n",
        "                        f\"  Outputs: {inv.get('outputs', '')}\\n\"\n",
        "                        f\"  Status: {inv.get('status', 'Unknown')}\"\n",
        "                    )\n",
        "        \n",
        "        return \"\\n\".join(invocations_lines) if invocations_lines else \"No tool invocations recorded\"\n",
        "    \n",
        "    def _compute_gsr(self, report: List[Dict[str, Any]]) -> Tuple[float, float]:\n",
        "        \"\"\"Compute Goal Success Rate metrics.\n",
        "        \n",
        "        Returns:\n",
        "            Tuple of (gsr, partial_gsr)\n",
        "            - gsr: Binary score (1.0 if ALL assertions True, else 0.0)\n",
        "            - partial_gsr: Percentage of True assertions\n",
        "        \"\"\"\n",
        "        if not report:\n",
        "            return 1.0, 1.0\n",
        "        \n",
        "        # Count True and False answers from LLM judgments\n",
        "        true_count = sum(1 for item in report if str(item.get(\"answer\", \"\")).lower() == \"true\")\n",
        "        false_count = sum(1 for item in report if str(item.get(\"answer\", \"\")).lower() == \"false\")\n",
        "        total = true_count + false_count\n",
        "        \n",
        "        if total == 0:\n",
        "            return 1.0, 1.0\n",
        "        \n",
        "        # GSR: 1.0 only if ALL assertions are True (strict)\n",
        "        gsr = 1.0 if false_count == 0 else 0.0\n",
        "        \n",
        "        # Partial GSR: percentage of True assertions (lenient)\n",
        "        partial_gsr = true_count / total\n",
        "        \n",
        "        return gsr, partial_gsr\n",
        "\n",
        "print(\"LLMAssertionEvaluator class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f352f0a",
      "metadata": {},
      "source": [
        "### How Evaluation Works in Practice\n",
        "\n",
        "Here's the evaluation flow during benchmark execution:\n",
        "\n",
        "**1. Agent runs and completes the task:**\n",
        "   - Interacts with simulated user\n",
        "   - Calls simulated tools\n",
        "   - Generates final answer\n",
        "\n",
        "**2. Benchmark collects traces:**\n",
        "   - `MessageHistory`: All agent-user messages\n",
        "   - Tool invocation history: Which tools were called, with what parameters\n",
        "\n",
        "**3. User-side evaluation:**\n",
        "   ```python\n",
        "   user_result = user_evaluator(trace=message_history)\n",
        "   # Returns: {\"gsr\": 1.0, \"partial_gsr\": 1.0, \"report\": [...]}\n",
        "   ```\n",
        "\n",
        "**4. System-side evaluation:**\n",
        "   ```python\n",
        "   system_result = system_evaluator(\n",
        "       trace=message_history, \n",
        "       tool_traces=environment.gather_traces()[\"tools\"]\n",
        "   )\n",
        "   # Returns: {\"gsr\": 0.0, \"partial_gsr\": 0.67, \"report\": [...]}\n",
        "   ```\n",
        "\n",
        "**5. Compute overall metrics:**\n",
        "   - **Overall GSR** = User GSR Ã— System GSR (both must be 1.0)\n",
        "   - **User GSR** = Did agent satisfy user's goals?\n",
        "   - **System GSR** = Did agent use tools correctly?\n",
        "   - **Partial GSRs** = Percentage of assertions met\n",
        "\n",
        "This dual evaluation ensures agents don't just satisfy users through incorrect meansâ€”they must also use the right tools with the right parameters!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a01e2b1",
      "metadata": {},
      "source": [
        "## Step 5: Implement the Benchmark Class\n",
        "\n",
        "Now we bring everything together in the `AWSCollabBenchmark` class! This class:\n",
        "\n",
        "1. **Extends** MASEval's `Benchmark` base class\n",
        "2. **Implements** required setup methods for environment, user, agents, and evaluators\n",
        "3. **Orchestrates** the entire benchmark execution flow\n",
        "4. **Uses minimal branching pattern**: Framework-agnostic logic with small framework-specific branches\n",
        "\n",
        "The base `Benchmark` class will call these methods automatically when you run `benchmark.run()`!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c36507a",
      "metadata": {},
      "source": [
        "The `AWSCollabBenchmark` class implements these key methods:\n",
        "\n",
        "**Setup Methods** (called for each task):\n",
        "- **`setup_environment()`**: Creates `TravelEnvironment` with simulated tools\n",
        "- **`setup_user()`**: Creates LLM-powered user simulator (`SmolAgentUser` or `LangGraphUser`)\n",
        "- **`setup_agents()`**: Creates multi-agent system (orchestrator + specialists)\n",
        "- **`setup_evaluators()`**: Creates user-side and system-side evaluators\n",
        "\n",
        "**Execution Methods**:\n",
        "- **`run_agents()`**: Executes agents on the task\n",
        "- **`evaluate()`**: Runs both evaluators and computes GSR metrics\n",
        "\n",
        "**Helper Methods**:\n",
        "- **`_create_langgraph_agent()`**: Creates LangGraph agent graph (framework-specific)\n",
        "\n",
        "Let's implement each method step by step!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d10049d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class AWSCollabBenchmark(Benchmark):\n",
        "    \"\"\"AWS Collaboration benchmark using minimal branching pattern.\"\"\"\n",
        "\n",
        "    def setup_environment(self, agent_data: Dict[str, Any], task: Task) -> Environment:\n",
        "        # Shared logic (framework-agnostic)\n",
        "        task_data = {\n",
        "            \"environment_data\": task.environment_data,\n",
        "            \"query\": task.query,\n",
        "            \"evaluation_data\": task.evaluation_data,\n",
        "            \"metadata\": task.metadata,\n",
        "        }\n",
        "        framework: str = agent_data[\"framework\"]\n",
        "        return TravelEnvironment(task_data, framework=framework, benchmark=self)\n",
        "\n",
        "    def setup_user(self, agent_data: Dict[str, Any], environment: Environment, task: Task) -> User:\n",
        "        # Shared preprocessing (framework-agnostic)\n",
        "        user_profile = task.environment_data.get(\"user_profile\", {})\n",
        "        scenario = task.metadata.get(\"scenario\", \"\")\n",
        "        initial_prompt = task.query\n",
        "\n",
        "        # Get framework from agent_data to create appropriate user type\n",
        "        framework = agent_data[\"framework\"]\n",
        "\n",
        "        # Framework-specific user class only\n",
        "        if framework == \"smolagents\":\n",
        "            return SmolAgentUser(\n",
        "                name=\"Simulated User\",\n",
        "                model=get_model(),\n",
        "                user_profile=user_profile,\n",
        "                scenario=scenario,\n",
        "                initial_prompt=initial_prompt,\n",
        "            )\n",
        "        elif framework == \"langgraph\":\n",
        "            return LangGraphUser(\n",
        "                name=\"Simulated User\",\n",
        "                model=get_model(),\n",
        "                user_profile=user_profile,\n",
        "                scenario=scenario,\n",
        "                initial_prompt=initial_prompt,\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported framework: {framework}\")\n",
        "\n",
        "    def setup_agents(\n",
        "        self, agent_data: Dict[str, Any], environment: Environment, task: Task, user: User | None\n",
        "    ) -> Tuple[List[AgentAdapter], Dict[str, AgentAdapter]]:\n",
        "        if not isinstance(environment, TravelEnvironment):\n",
        "            raise TypeError(\"Expected TravelEnvironment\")\n",
        "\n",
        "        # Shared preprocessing (framework-agnostic)\n",
        "        framework = agent_data[\"framework\"]\n",
        "        primary_agent_id = agent_data[\"primary_agent_id\"]\n",
        "        primary_spec = next(a for a in agent_data[\"agents\"] if a[\"agent_id\"] == primary_agent_id)\n",
        "        \n",
        "        # Extract model config from agent spec\n",
        "        model_config = primary_spec.get(\"model_config\", {})\n",
        "        model_id = model_config.get(\"model_id\", \"gemini-2.5-flash\")\n",
        "        temperature = model_config.get(\"temperature\", 0.7)\n",
        "\n",
        "        # Framework-specific agent creation only       \n",
        "        if framework == \"smolagents\":\n",
        "            # Initialize smolagents model\n",
        "            smolagents_model = OpenAIServerModel(\n",
        "                model_id=model_id,\n",
        "                api_base=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        "                api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
        "            )\n",
        "\n",
        "            # Create specialist agents (sub-agents)\n",
        "            specialist_agents = []\n",
        "            for agent_spec in agent_data[\"agents\"]:\n",
        "                if agent_spec[\"agent_id\"] == primary_agent_id:\n",
        "                    continue  # Skip the primary agent itself\n",
        "                \n",
        "                # Get tools for this specialist\n",
        "                specialist_tools = environment.get_tools_for_agent(agent_spec.get(\"tools\", []))\n",
        "                specialist_tools.append(FinalAnswerTool())\n",
        "                \n",
        "                # Create specialist agent\n",
        "                specialist = ToolCallingAgent(\n",
        "                    model=smolagents_model,\n",
        "                    tools=specialist_tools,\n",
        "                    name=agent_spec[\"agent_name\"],\n",
        "                    description=agent_spec[\"agent_instruction\"],\n",
        "                    verbosity_level=0,\n",
        "                )\n",
        "                specialist_agents.append(specialist)\n",
        "\n",
        "            # Get primary agent tools (usually empty for orchestrators)\n",
        "            primary_agent_tools = environment.get_tools_for_agent(primary_spec.get(\"tools\", []))\n",
        "            \n",
        "            # Build primary agent tool list\n",
        "            tools = primary_agent_tools + [FinalAnswerTool()]\n",
        "            user_tool = user.get_tool() if user else None\n",
        "            if user_tool:\n",
        "                tools.append(user_tool)\n",
        "\n",
        "            # Create primary orchestrator agent with managed_agents\n",
        "            agent = ToolCallingAgent(\n",
        "                model=smolagents_model,\n",
        "                tools=tools,\n",
        "                managed_agents=specialist_agents if specialist_agents else None,\n",
        "                name=primary_spec[\"agent_name\"],\n",
        "                description=primary_spec[\"agent_instruction\"],\n",
        "                verbosity_level=0,\n",
        "            )\n",
        "\n",
        "            wrapper = SmolAgentAdapter(agent, primary_agent_id)\n",
        "\n",
        "        elif framework == \"langgraph\":\n",
        "            # Initialize LangChain model\n",
        "            langgraph_model = ChatGoogleGenerativeAI(\n",
        "                model=model_id,\n",
        "                google_api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
        "                temperature=temperature,\n",
        "            )\n",
        "\n",
        "            # Get primary agent tools\n",
        "            primary_agent_tools = environment.get_tools_for_agent(primary_spec.get(\"tools\", []))\n",
        "            user_tool = user.get_tool() if user else None\n",
        "            \n",
        "            # Build tool list\n",
        "            tools = primary_agent_tools[:]\n",
        "            if user_tool:\n",
        "                tools.append(user_tool)\n",
        "\n",
        "            # Create agent graph\n",
        "            graph = self._create_langgraph_agent(\n",
        "                model=langgraph_model,\n",
        "                tools=tools,\n",
        "                agent_name=primary_spec[\"agent_name\"],\n",
        "                agent_instruction=primary_spec[\"agent_instruction\"],\n",
        "            )\n",
        "\n",
        "            wrapper = LangGraphAgentAdapter(graph, primary_agent_id)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported framework: {framework}\")\n",
        "\n",
        "        agents_dict: Dict[str, AgentAdapter] = {primary_agent_id: wrapper}\n",
        "        return [wrapper], agents_dict\n",
        "\n",
        "    def _create_langgraph_agent(self, model: Any, tools: List[Any], agent_name: str, agent_instruction: str):\n",
        "        \"\"\"Helper method to create a LangGraph agent (only called for langgraph framework).\"\"\"\n",
        "        class AgentState(TypedDict):\n",
        "            messages: List[Any]\n",
        "\n",
        "        # Bind tools to LLM\n",
        "        llm_with_tools = model.bind_tools(tools)\n",
        "\n",
        "        # Define the agent node\n",
        "        def call_model(state: AgentState):\n",
        "            messages = state[\"messages\"]\n",
        "            # Add system message with agent instruction if not present\n",
        "            has_system = any(isinstance(m, SystemMessage) for m in messages)\n",
        "            if not has_system:\n",
        "                system_message = SystemMessage(content=f\"{agent_name}: {agent_instruction}\")\n",
        "                messages = [system_message] + messages\n",
        "            \n",
        "            response = llm_with_tools.invoke(messages)\n",
        "            return {\"messages\": [response]}\n",
        "\n",
        "        # Build the graph\n",
        "        workflow = StateGraph(AgentState)\n",
        "        workflow.add_node(\"agent\", call_model)\n",
        "        workflow.add_node(\"tools\", ToolNode(tools))\n",
        "        workflow.set_entry_point(\"agent\")\n",
        "        workflow.add_conditional_edges(\"agent\", tools_condition, {\"tools\": \"tools\", END: END})\n",
        "        workflow.add_edge(\"tools\", \"agent\")\n",
        "\n",
        "        return workflow.compile()\n",
        "\n",
        "    def setup_evaluators(self, environment, task, agents, user) -> Sequence[Evaluator]:\n",
        "        \"\"\"Create both user-side and system-side evaluators.\"\"\"\n",
        "        user_evaluator = LLMAssertionEvaluator(\n",
        "            get_model(), task, gsr_type=\"user\"\n",
        "        )\n",
        "        system_evaluator = LLMAssertionEvaluator(\n",
        "            get_model(), task, gsr_type=\"system\"\n",
        "        )\n",
        "        return [user_evaluator, system_evaluator]\n",
        "\n",
        "    def run_agents(self, agents: Sequence[AgentAdapter], task: Task, environment: Environment) -> Any:\n",
        "        # Execute agents and return their final answers (not full traces)\n",
        "        answers = [agent.run(task.query) for agent in agents]\n",
        "        return answers[0] if len(answers) == 1 else answers\n",
        "\n",
        "    def evaluate(\n",
        "        self,\n",
        "        evaluators: Sequence[Evaluator],\n",
        "        agents: Dict[str, AgentAdapter],\n",
        "        final_answer: Any,\n",
        "        traces: Dict[str, Any],\n",
        "    ) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Evaluate using both user-side and system-side evaluators.\n",
        "        \n",
        "        Following AWS Multi-Agent Collaboration paper (https://arxiv.org/html/2412.05449v1):\n",
        "        - User-side assertions: Evaluate behaviors observable by the user (conversation only)\n",
        "        - System-side assertions: Evaluate behaviors NOT observable by user (tool calls, internal state)\n",
        "        \n",
        "        Returns aggregated evaluation results matching AWS paper format:\n",
        "        - user_gsr: Goal Success Rate from user perspective\n",
        "        - system_gsr: Goal Success Rate from system perspective  \n",
        "        - overall_gsr: Combined GSR (both must pass)\n",
        "        - supervisor_gsr: Supervisor agent success rate (user-side OR overall success)\n",
        "        - partial_gsr: Percentage of all assertions that passed\n",
        "        - user_partial_gsr: Percentage of user assertions that passed (bonus metric)\n",
        "        - system_partial_gsr: Percentage of system assertions that passed (bonus metric)\n",
        "        - report: Combined list of all assertion judgments\n",
        "        \"\"\"\n",
        "        # Use user traces for user-observable conversation (already filtered by User class)\n",
        "        # The user's history only contains messages from user_input tool interactions\n",
        "        user_trace = traces.get(\"user\", {})\n",
        "        user_observable_messages = MessageHistory(user_trace.get(\"history\", []))\n",
        "        \n",
        "        # Extract primary agent's full message history for system-side evaluation\n",
        "        primary_agent_id = list(agents.keys())[0]\n",
        "        agent_trace = traces.get(\"agents\", {}).get(primary_agent_id, {})\n",
        "        all_messages = MessageHistory(agent_trace.get(\"messages\", []))\n",
        "        \n",
        "        # Extract tool invocations from top-level tools traces (registered directly)\n",
        "        tool_traces = traces.get(\"tools\", {})\n",
        "        \n",
        "        # Run both evaluators\n",
        "        results = []\n",
        "        for evaluator in evaluators:\n",
        "            # Pass tool traces to system evaluator, user-observable messages to user evaluator\n",
        "            if isinstance(evaluator, LLMAssertionEvaluator) and evaluator.gsr_type == \"system\":\n",
        "                # System evaluator gets full message history + tool traces\n",
        "                result = evaluator(all_messages, tool_traces)\n",
        "            else:\n",
        "                # User evaluator gets only user-observable messages\n",
        "                result = evaluator(user_observable_messages)\n",
        "            results.append(result)\n",
        "        \n",
        "        # Combine results (first is user, second is system based on setup_evaluators order)\n",
        "        user_result = results[0] if len(results) > 0 else {\"gsr\": 0.0, \"partial_gsr\": 0.0, \"report\": []}\n",
        "        system_result = results[1] if len(results) > 1 else {\"gsr\": 0.0, \"partial_gsr\": 0.0, \"report\": []}\n",
        "        \n",
        "        # Compute overall metrics\n",
        "        combined_report = user_result.get(\"report\", []) + system_result.get(\"report\", [])\n",
        "        \n",
        "        # Overall GSR: Both user and system must have GSR=1.0\n",
        "        overall_gsr = 1.0 if (user_result.get(\"gsr\", 0.0) == 1.0 and system_result.get(\"gsr\", 0.0) == 1.0) else 0.0\n",
        "        \n",
        "        # Supervisor GSR: Per AWS paper Table 1 - \"If overall GSR is 1 or supervisor agent is reliable, then score is 1; else 0\"\n",
        "        # Interpretation: Supervisor is considered successful if overall task succeeds OR if supervisor did its job correctly\n",
        "        # (i.e., user-side assertions passed, meaning supervisor communicated correctly with user regardless of sub-agent failures)\n",
        "        supervisor_gsr = 1.0 if (overall_gsr == 1.0 or user_result.get(\"gsr\", 0.0) == 1.0) else 0.0\n",
        "        \n",
        "        # Overall partial GSR: percentage of ALL assertions that passed\n",
        "        if combined_report:\n",
        "            total_true = sum(1 for item in combined_report if str(item.get(\"answer\", \"\")).lower() == \"true\")\n",
        "            total_assertions = len(combined_report)\n",
        "            overall_partial_gsr = total_true / total_assertions if total_assertions > 0 else 1.0\n",
        "        else:\n",
        "            overall_partial_gsr = 1.0\n",
        "        \n",
        "        # Return in AWS format\n",
        "        return [{\n",
        "            \"user_gsr\": user_result.get(\"gsr\", 0.0),\n",
        "            \"user_partial_gsr\": user_result.get(\"partial_gsr\", 0.0),\n",
        "            \"system_gsr\": system_result.get(\"gsr\", 0.0),\n",
        "            \"system_partial_gsr\": system_result.get(\"partial_gsr\", 0.0),\n",
        "            \"overall_gsr\": overall_gsr,\n",
        "            \"overall_partial_gsr\": overall_partial_gsr,\n",
        "            \"supervisor_gsr\": supervisor_gsr,\n",
        "            \"report\": combined_report,\n",
        "        }]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3b81d0b",
      "metadata": {},
      "source": [
        "# Step 6: Run Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30e13c7d",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "benchmark = AWSCollabBenchmark(agent_data=agent_config, tasks=tasks, callbacks=[logger])\n",
        "results = benchmark.run()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41821fa9",
      "metadata": {},
      "source": [
        "## Step 7: Get final results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "816d1160",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def compute_benchmark_metric(results):\n",
        "    \"\"\"\n",
        "    Compute the benchmark summary and mean of each numeric metric across all results.\n",
        "    Returns a dict with summary and mean metrics for printing.\n",
        "    \"\"\"\n",
        "    if not results:\n",
        "        return {\n",
        "            \"total_tasks\": 0,\n",
        "            \"successful_tasks\": 0,\n",
        "            \"success_rate\": 0.0,\n",
        "            \"mean_metrics\": {},\n",
        "        }\n",
        "    total_tasks = len(results)\n",
        "    # Aggregate metrics from nested 'eval' list in each result\n",
        "    metric_sums = {}\n",
        "    metric_counts = {}\n",
        "    successful_tasks = 0\n",
        "\n",
        "    for res in results:\n",
        "        evals = res.get('eval', [])\n",
        "        # For success rate, use 'overall_gsr' if present in any eval entry\n",
        "        found_success = False\n",
        "        for entry in evals:\n",
        "            for k, v in entry.items():\n",
        "                if isinstance(v, (int, float)):\n",
        "                    metric_sums.setdefault(k, 0.0)\n",
        "                    metric_counts.setdefault(k, 0)\n",
        "                    metric_sums[k] += v\n",
        "                    metric_counts[k] += 1\n",
        "            if not found_success and entry.get('overall_gsr', 0.0) == 1.0:\n",
        "                found_success = True\n",
        "        if found_success:\n",
        "            successful_tasks += 1\n",
        "\n",
        "    success_rate = successful_tasks / total_tasks if total_tasks > 0 else 0.0\n",
        "    mean_metrics = {k: (metric_sums[k] / metric_counts[k] if metric_counts[k] else 0.0) for k in metric_sums}\n",
        "    return {\n",
        "        \"total_tasks\": total_tasks,\n",
        "        \"successful_tasks\": successful_tasks,\n",
        "        \"success_rate\": success_rate,\n",
        "        \"mean_metrics\": mean_metrics,\n",
        "    }\n",
        "\n",
        "# Compute benchmark summary and mean metrics\n",
        "summary = compute_benchmark_metric(results)\n",
        "\n",
        "# Print summary\n",
        "print(f\"\\n--- Benchmark Summary ---\")\n",
        "print(f\"Total Tasks: {summary['total_tasks']}\")\n",
        "print(f\"Successful Tasks (Overall GSR=1.0): {summary['successful_tasks']}\")\n",
        "print(f\"Success Rate: {summary['success_rate']:.2%}\")\n",
        "\n",
        "print(\"\\nMean metrics across all tasks:\")\n",
        "for k, v in summary['mean_metrics'].items():\n",
        "    print(f\"  {k:<20} {v:.4f}\")\n",
        "\n",
        "# 6. Print final results\n",
        "print(\"\\n--- Benchmark Complete ---\")\n",
        "print(f\"Results saved to: {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c7c1d6c",
      "metadata": {},
      "source": [
        "## Closing Remarks\n",
        "\n",
        "This notebook demonstrated implementing the AWS Collaboration benchmark using MASEval's orchestration framework. By extending the `Benchmark` base class and implementing six abstract methods (`setup_environment`, `setup_user`, `setup_agents`, `setup_evaluators`, `run_agents`, `evaluate`), we created a complete multi-agent evaluation pipeline. The framework handled task iteration, trace collection, configuration gathering, and result aggregation automaticallyâ€”we focused purely on domain-specific logic.\n",
        "\n",
        "**Key patterns demonstrated:**\n",
        "- **Minimal branching pattern**: Framework-agnostic core logic with small smolagents/langgraph branches only at instantiation\n",
        "- **LLM-powered simulation**: Realistic tools, user interactions, and evaluation without real APIs or mock data\n",
        "- **Dual evaluation**: Separate user-side (observable behavior) and system-side (tool correctness) assertions for comprehensive assessment\n",
        "- **Composition over inheritance**: `GenericTool` wrapped by framework-specific adapters to avoid method signature conflicts\n",
        "- **Automatic tracing & config**: Registry pattern collects execution traces and configurations from all components for reproducibility\n",
        "- **Callback extensibility**: `FileResultLogger` demonstrates lifecycle hooks for custom logging, metrics, or external integrations\n",
        "- **Strict core/interface separation**: `maseval/core` has minimal dependencies; framework adapters live in `maseval/interface` with optional dependencies\n",
        "\n",
        "All of these patterns are **optional**. MASEval is very flexible and can be used in many ways with real tools, real users, MCP servers etc. This is just an example to demonstrate one possible workflow!\n",
        "\n",
        "**Contributing:** MASEval is designed to be extended. We welcome contributionsâ€”whether adding new framework adapters, implementing domain-specific benchmarks, improving documentation, or suggesting architectural improvements. Open an issue or PR on GitHub, or reach out with feedback on design patterns and use cases you'd like to see supported!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
