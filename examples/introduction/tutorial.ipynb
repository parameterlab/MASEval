{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0944cb39",
   "metadata": {},
   "source": [
    "# Tiny Tutorial\n",
    "\n",
    "[![Open Notebook on GitHub](https://img.shields.io/badge/Open%20Notebook%20on-GitHub-blue?logo=github)](https://github.com/parameterlab/MASEval/blob/main/examples/introduction/tutorial.ipynb)\n",
    "\n",
    "This notebook is available as a Jupyter notebook — clone the repo and run it yourself!\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- **Build your first agent** — Create tools and agents with smolagents\n",
    "- **Run a minimal benchmark** — One task, one agent, end-to-end\n",
    "- **Understand the core abstractions** — Tasks, Environments, Evaluators working together\n",
    "\n",
    "\n",
    "This tutorial first introduces [`smolagents`](https://huggingface.co/docs/smolagents/en/index) as introduction to agents. Then it provides a super small single task benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb11872",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required dependencies and import the libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c1e065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install maseval[smolagents]\n",
    "# !pip install litellm\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "# Set your API key\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"your-api-key-here\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47dc994",
   "metadata": {},
   "source": [
    "## Part 1: Agent Initialization with smolagents\n",
    "\n",
    "Let's start by building an agent using smolagents. We'll create a simple agent that can handle email and banking tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3030fd",
   "metadata": {},
   "source": [
    "### 1.1 Define Custom Tools\n",
    "\n",
    "For this example, we'll create simplified versions of email and banking tools. In the full benchmark, these tools are more sophisticated and stateful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dc1ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import Tool\n",
    "\n",
    "class SimpleBankingTool(Tool):\n",
    "    \"\"\"A simple tool to retrieve banking transactions.\"\"\"\n",
    "    \n",
    "    name = \"get_transactions\"\n",
    "    description = \"Retrieve recent banking transactions. Returns a list of transactions with date, description, amount, and type.\"\n",
    "    inputs = {}\n",
    "    output_type = \"string\"\n",
    "    \n",
    "    def __init__(self, transactions: List[Dict], **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.transactions = transactions\n",
    "    \n",
    "    def forward(self) -> str:\n",
    "        \"\"\"Return all transactions as formatted string.\"\"\"\n",
    "        if not self.transactions:\n",
    "            return \"No transactions found.\"\n",
    "        \n",
    "        result = \"Recent Transactions:\\n\"\n",
    "        for txn in self.transactions:\n",
    "            result += f\"- {txn['date']}: {txn['description']} - ${txn['amount']} ({txn['type']})\\n\"\n",
    "        return result\n",
    "\n",
    "\n",
    "class SimpleEmailTool(Tool):\n",
    "    \"\"\"A simple tool to send emails.\"\"\"\n",
    "    \n",
    "    name = \"send_email\"\n",
    "    description = \"Send an email to a recipient. Provide the recipient email, subject, and body text.\"\n",
    "    inputs = {\n",
    "        \"to\": {\"type\": \"string\", \"description\": \"Recipient email address\"},\n",
    "        \"subject\": {\"type\": \"string\", \"description\": \"Email subject line\"},\n",
    "        \"body\": {\"type\": \"string\", \"description\": \"Email body text\"}\n",
    "    }\n",
    "    output_type = \"string\"\n",
    "    \n",
    "    def __init__(self, sent_emails: List, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.sent_emails = sent_emails  # Store sent emails for tracking\n",
    "    \n",
    "    def forward(self, to: str, subject: str, body: str) -> str:\n",
    "        \"\"\"Send an email and store it.\"\"\"\n",
    "        email = {\"to\": to, \"subject\": subject, \"body\": body}\n",
    "        self.sent_emails.append(email)\n",
    "        return f\"Email sent successfully to {to}\"\n",
    "\n",
    "print(\"Tools defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5fabec",
   "metadata": {},
   "source": [
    "### 1.2 Create Tool Instances with Data\n",
    "\n",
    "Now let's instantiate our tools with the actual data from the benchmark task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faad0063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample banking data from the benchmark\n",
    "banking_transactions = [\n",
    "    {\n",
    "        \"date\": \"2025-11-15\",\n",
    "        \"description\": \"Tenant Deposit - Sarah Johnson\",\n",
    "        \"amount\": 2000,\n",
    "        \"type\": \"deposit\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-11-17\",\n",
    "        \"description\": \"Rent Payment - Sarah Johnson\",\n",
    "        \"amount\": 1500,\n",
    "        \"type\": \"deposit\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-11-16\",\n",
    "        \"description\": \"Property Maintenance\",\n",
    "        \"amount\": -450,\n",
    "        \"type\": \"expense\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# List to track sent emails\n",
    "sent_emails = []\n",
    "\n",
    "# Create tool instances\n",
    "banking_tool = SimpleBankingTool(transactions=banking_transactions)\n",
    "email_tool = SimpleEmailTool(sent_emails=sent_emails)\n",
    "\n",
    "print(f\"Created {len([banking_tool, email_tool])} tools\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f15e53a",
   "metadata": {},
   "source": [
    "### 1.3 Initialize the Agent\n",
    "\n",
    "Now we'll create a smolagents agent with our custom tools and give it clear instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c758c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import ToolCallingAgent, LiteLLMModel\n",
    "\n",
    "# Initialize the model\n",
    "model = LiteLLMModel(\n",
    "    model_id=\"gemini/gemini-2.5-flash\",\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Create the agent with tools and instructions\n",
    "agent = ToolCallingAgent(\n",
    "    tools=[banking_tool, email_tool],\n",
    "    model=model,\n",
    "    instructions=\"\"\"You are a helpful assistant that helps users with email and banking tasks.\n",
    "Use the available tools to retrieve information and take appropriate actions.\n",
    "Be professional and thorough in your responses.\"\"\"\n",
    ")\n",
    "\n",
    "print(\"Agent initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526721ea",
   "metadata": {},
   "source": [
    "### 1.4 Test the Agent\n",
    "\n",
    "Let's test our agent with the actual task query from the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3217b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The task query from the benchmark\n",
    "query = \"\"\"Sarah Johnson emailed me to confirm that I received her payment for the deposit \n",
    "and first month's rent. Please check my transactions and send an email reply accordingly.\"\"\"\n",
    "\n",
    "# Run the agent\n",
    "response = agent.run(query)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AGENT RESPONSE:\")\n",
    "print(\"=\"*60)\n",
    "print(response)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd59497d",
   "metadata": {},
   "source": [
    "### 1.5 Inspect What Happened\n",
    "\n",
    "Let's check if the agent sent an email and what it contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3022904",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Emails sent by the agent:\")\n",
    "print(\"\\n\")\n",
    "\n",
    "if sent_emails:\n",
    "    for i, email in enumerate(sent_emails, 1):\n",
    "        print(f\"Email #{i}\")\n",
    "        print(f\"To: {email['to']}\")\n",
    "        print(f\"Subject: {email['subject']}\")\n",
    "        print(f\"Body:\\n{email['body']}\")\n",
    "        print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "else:\n",
    "    print(\"No emails were sent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c45159b",
   "metadata": {},
   "source": [
    "## Part 2: Evaluating Agents with MASEval\n",
    "\n",
    "Now that we understand how the agent works, let's see how MASEval helps us systematically evaluate agent performance across multiple tasks.\n",
    "\n",
    "MASEval provides:\n",
    "- **Tasks**: Define queries, environments, and evaluation criteria\n",
    "- **Environments**: Manage tool state and provide context\n",
    "- **Evaluators**: Measure agent performance using various metrics\n",
    "- **Benchmarks**: Orchestrate execution and collect results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a0d0d2",
   "metadata": {},
   "source": [
    "### 2.1 Import MASEval Components\n",
    "\n",
    "Let's import the core MASEval components we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e91c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from maseval import Benchmark, Environment, Evaluator, Task, TaskCollection\n",
    "from maseval.interface.agents.smolagents import SmolAgentAdapter\n",
    "\n",
    "print(\"MASEval components imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4570828d",
   "metadata": {},
   "source": [
    "### 2.2 Load Task Data\n",
    "\n",
    "The Five-A-Day benchmark uses JSON files to define tasks. Let's load the first task (Email & Banking)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d76f49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load task data from JSON\n",
    "data_dir = Path(\"data\")\n",
    "\n",
    "with open(data_dir / \"tasks.json\", \"r\") as f:\n",
    "    tasks_data = json.load(f)\n",
    "\n",
    "# Get the first task (Email & Banking)\n",
    "task_data = tasks_data[0]\n",
    "\n",
    "print(\"Task Query:\")\n",
    "print(task_data[\"query\"])\n",
    "print(\"\\nTools Required:\")\n",
    "print(task_data[\"environment_data\"][\"tools\"])\n",
    "print(\"\\nEvaluators:\")\n",
    "print(task_data[\"evaluation_data\"][\"evaluators\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2e7375",
   "metadata": {},
   "source": [
    "### 2.3 Create a Task Object\n",
    "\n",
    "MASEval uses `Task` objects to encapsulate all information about a benchmark task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5498a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Task instance\n",
    "task = Task(\n",
    "    query=task_data[\"query\"],\n",
    "    environment_data=task_data[\"environment_data\"],\n",
    "    evaluation_data=task_data[\"evaluation_data\"],\n",
    "    metadata=task_data[\"metadata\"]\n",
    ")\n",
    "\n",
    "print(f\"Created task: {task.metadata['task_id']}\")\n",
    "print(f\"Complexity: {task.metadata['complexity']}\")\n",
    "print(f\"Skills tested: {', '.join(task.metadata['skills_tested'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8571fed7",
   "metadata": {},
   "source": [
    "### 2.4 Define a Custom Environment\n",
    "\n",
    "The `Environment` class manages tool state and provides tools to the agent. Here's a simplified version of the FiveADayEnvironment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4240e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEnvironment(Environment):\n",
    "    \"\"\"Simplified environment for the Email & Banking task.\"\"\"\n",
    "    \n",
    "    def setup_state(self, task_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Initialize environment state from task data.\"\"\"\n",
    "        return task_data.copy()\n",
    "    \n",
    "    def create_tools(self) -> list:\n",
    "        \"\"\"Create tool instances from environment data.\"\"\"\n",
    "        # Get banking transactions from environment data\n",
    "        transactions = self.state.get(\"banking\", {}).get(\"bank_transactions\", [])\n",
    "        \n",
    "        # Create tool instances - track sent emails for evaluation\n",
    "        self.sent_emails: List[Dict] = []\n",
    "        banking_tool = SimpleBankingTool(transactions=transactions)\n",
    "        email_tool = SimpleEmailTool(sent_emails=self.sent_emails)\n",
    "        \n",
    "        return [banking_tool, email_tool]\n",
    "\n",
    "print(\"Environment class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9115c50b",
   "metadata": {},
   "source": [
    "### 2.5 Create Custom Evaluators\n",
    "\n",
    "Evaluators measure agent performance. Let's create two evaluators:\n",
    "1. **FinancialAccuracyEvaluator**: Checks if the agent verified the correct payment amounts\n",
    "2. **EmailSentEvaluator**: Checks if the agent sent an email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f43d212",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialAccuracyEvaluator(Evaluator):\n",
    "    \"\"\"Evaluates if the agent correctly identified payment amounts.\"\"\"\n",
    "    \n",
    "    def __init__(self, task: Task, environment: Environment, user=None):\n",
    "        \"\"\"Initialize with task, environment, and optional user.\"\"\"\n",
    "        super().__init__(task, environment, user)\n",
    "        self.task = task\n",
    "        self.environment = environment\n",
    "    \n",
    "    def filter_traces(self, traces: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Filter to environment traces to check tool usage.\"\"\"\n",
    "        return traces.get(\"environment\", {})\n",
    "    \n",
    "    def __call__(self, traces: Dict[str, Any], final_answer: Optional[str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Check if banking information was accessed and email was sent.\"\"\"\n",
    "        # Expected values from task evaluation data\n",
    "        expected_deposit = self.task.evaluation_data[\"expected_deposit_amount\"]\n",
    "        expected_rent = self.task.evaluation_data[\"expected_rent_amount\"]\n",
    "        \n",
    "        # Check if emails were sent by looking at environment state\n",
    "        sent_emails = getattr(self.environment, 'sent_emails', [])\n",
    "        email_sent = len(sent_emails) > 0\n",
    "        \n",
    "        return {\n",
    "            \"evaluator\": \"FinancialAccuracyEvaluator\",\n",
    "            \"email_sent\": email_sent,\n",
    "            \"emails_count\": len(sent_emails),\n",
    "            \"expected_deposit\": expected_deposit,\n",
    "            \"expected_rent\": expected_rent,\n",
    "            \"score\": 1.0 if email_sent else 0.0,\n",
    "            \"message\": \"Agent sent confirmation email\" if email_sent else \"No email was sent\"\n",
    "        }\n",
    "\n",
    "\n",
    "class EmailSentEvaluator(Evaluator):\n",
    "    \"\"\"Evaluates if the agent sent an email with proper content.\"\"\"\n",
    "    \n",
    "    def __init__(self, task: Task, environment: Environment, user=None):\n",
    "        \"\"\"Initialize with task, environment, and optional user.\"\"\"\n",
    "        super().__init__(task, environment, user)\n",
    "        self.task = task\n",
    "        self.environment = environment\n",
    "    \n",
    "    def filter_traces(self, traces: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Filter to environment traces.\"\"\"\n",
    "        return traces.get(\"environment\", {})\n",
    "    \n",
    "    def __call__(self, traces: Dict[str, Any], final_answer: Optional[str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Check if email was sent with appropriate content.\"\"\"\n",
    "        sent_emails = getattr(self.environment, 'sent_emails', [])\n",
    "        \n",
    "        if not sent_emails:\n",
    "            return {\n",
    "                \"evaluator\": \"EmailSentEvaluator\",\n",
    "                \"email_sent\": False,\n",
    "                \"score\": 0.0,\n",
    "                \"error\": \"No email was sent\"\n",
    "            }\n",
    "        \n",
    "        # Get the last email that was sent\n",
    "        email_data = sent_emails[-1]\n",
    "        \n",
    "        return {\n",
    "            \"evaluator\": \"EmailSentEvaluator\",\n",
    "            \"email_sent\": True,\n",
    "            \"score\": 1.0,\n",
    "            \"recipient\": email_data.get(\"to\"),\n",
    "            \"subject\": email_data.get(\"subject\"),\n",
    "            \"message\": \"Agent successfully sent an email\"\n",
    "        }\n",
    "\n",
    "print(\"Evaluators defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aab59f",
   "metadata": {},
   "source": [
    "### 2.6 Create a Custom Benchmark\n",
    "\n",
    "The `Benchmark` class orchestrates task execution and evaluation. We'll create a simplified version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88f62d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from maseval import AgentAdapter\n",
    "from typing import Sequence, Tuple\n",
    "\n",
    "class SimpleBenchmark(Benchmark):\n",
    "    \"\"\"Simplified benchmark for the tutorial.\"\"\"\n",
    "    \n",
    "    def setup_environment(self, agent_data: Dict[str, Any], task: Task) -> Environment:\n",
    "        \"\"\"Create an environment for the task.\"\"\"\n",
    "        return SimpleEnvironment(task.environment_data)\n",
    "    \n",
    "    def setup_agents(\n",
    "        self,\n",
    "        agent_data: Dict[str, Any],\n",
    "        environment: Environment,\n",
    "        task: Task,\n",
    "        user=None\n",
    "    ) -> Tuple[Sequence[AgentAdapter], Dict[str, AgentAdapter]]:\n",
    "        \"\"\"Create an agent for the task.\"\"\"\n",
    "        # Initialize model\n",
    "        model = LiteLLMModel(\n",
    "            model_id=\"gemini/gemini-2.5-flash\",\n",
    "            api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        # Create agent with environment tools\n",
    "        agent = ToolCallingAgent(\n",
    "            tools=environment.get_tools(),\n",
    "            model=model,\n",
    "            instructions=\"\"\"You are a helpful assistant. Help users with email and banking tasks \n",
    "by using the available tools to retrieve information and take appropriate actions. \n",
    "Be professional and thorough in your responses.\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Wrap agent in adapter for MASEval\n",
    "        agent_adapter = SmolAgentAdapter(agent, \"main_agent\")\n",
    "        \n",
    "        # Return (agents_to_run, agents_dict)\n",
    "        return [agent_adapter], {\"main_agent\": agent_adapter}\n",
    "    \n",
    "    def setup_evaluators(\n",
    "        self,\n",
    "        environment: Environment,\n",
    "        task: Task,\n",
    "        agents: Sequence[AgentAdapter],\n",
    "        user=None\n",
    "    ) -> Sequence[Evaluator]:\n",
    "        \"\"\"Create evaluators for the task.\"\"\"\n",
    "        return [\n",
    "            FinancialAccuracyEvaluator(task, environment, user),\n",
    "            EmailSentEvaluator(task, environment, user)\n",
    "        ]\n",
    "    \n",
    "    def run_agents(\n",
    "        self,\n",
    "        agents: Sequence[AgentAdapter],\n",
    "        task: Task,\n",
    "        environment: Environment\n",
    "    ) -> Any:\n",
    "        \"\"\"Execute the agent and return the final answer.\"\"\"\n",
    "        # Run the main agent with the task query\n",
    "        agent = agents[0]\n",
    "        result = agent.run(task.query)\n",
    "        return result\n",
    "    \n",
    "    def evaluate(\n",
    "        self,\n",
    "        evaluators: Sequence[Evaluator],\n",
    "        agents: Dict[str, AgentAdapter],\n",
    "        final_answer: Any,\n",
    "        traces: Dict[str, Any]\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Evaluate agent performance.\"\"\"\n",
    "        results = []\n",
    "        for evaluator in evaluators:\n",
    "            # Filter traces for this evaluator\n",
    "            filtered_traces = evaluator.filter_traces(traces)\n",
    "            # Run evaluation\n",
    "            result = evaluator(filtered_traces, final_answer)\n",
    "            results.append(result)\n",
    "        return results\n",
    "\n",
    "print(\"Benchmark class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c42385",
   "metadata": {},
   "source": [
    "### 2.7 Run the Benchmark\n",
    "\n",
    "Now let's run the benchmark on our task and see the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ee60a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create benchmark instance with agent configuration\n",
    "agent_data = {\n",
    "    \"model_id\": \"gemini/gemini-2.5-flash\",\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "benchmark = SimpleBenchmark(agent_data=agent_data, progress_bar=False)\n",
    "\n",
    "# Create task collection\n",
    "tasks = TaskCollection([task])\n",
    "\n",
    "# Run the benchmark\n",
    "print(\"Running benchmark...\\n\")\n",
    "reports = benchmark.run(tasks=tasks)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BENCHMARK COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a09ca38",
   "metadata": {},
   "source": [
    "### 2.8 Analyze the Results\n",
    "\n",
    "Let's examine the evaluation results to see how well our agent performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a725118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get results for the first (and only) task\n",
    "report = reports[0]\n",
    "\n",
    "print(f\"Task ID: {report['task_id']}\")\n",
    "print(f\"Status: {report['status']}\")\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if report.get(\"eval\"):\n",
    "    for eval_result in report[\"eval\"]:\n",
    "        print(f\"\\nEvaluator: {eval_result.get('evaluator', 'Unknown')}\")\n",
    "        print(f\"Score: {eval_result.get('score', 'N/A')}\")\n",
    "        \n",
    "        # Print relevant details\n",
    "        for key, value in eval_result.items():\n",
    "            if key not in [\"evaluator\", \"score\"]:\n",
    "                print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(\"No evaluation results available.\")\n",
    "    if report.get(\"error\"):\n",
    "        print(f\"\\nError: {report['error']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefd287a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "\n",
    "### Part 1: Agent Development\n",
    "- How to create custom tools for smolagents\n",
    "- How to initialize and configure a ToolCallingAgent\n",
    "- How to test your agent with queries\n",
    "\n",
    "### Part 2: Systematic Evaluation with MASEval\n",
    "- How to structure tasks with queries, environments, and evaluation criteria\n",
    "- How to create custom environments that manage tool state\n",
    "- How to write evaluators that measure specific aspects of agent performance\n",
    "- How to run benchmarks and analyze results\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Try the Five-A-Day Benchmark notebook** — A production-ready example with multi-agent systems and diverse evaluators\n",
    "2. Create your own custom evaluators for your specific use case\n",
    "3. Experiment with different agent frameworks (LangGraph, LlamaIndex)\n",
    "4. Add callbacks for logging and tracing\n",
    "\n",
    "For more information, visit the [MASEval documentation](https://github.com/parameterlab/MASEval)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
