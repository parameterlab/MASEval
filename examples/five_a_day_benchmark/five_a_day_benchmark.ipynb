{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fd80dc4",
   "metadata": {},
   "source": [
    "# 5 A Day Benchmark\n",
    "\n",
    "[![Open Notebook on GitHub](https://img.shields.io/badge/Open%20Notebook%20on-GitHub-blue?logo=github)](https://github.com/parameterlab/MASEval/blob/main/examples/five_a_day_benchmark/five_a_day_benchmark.ipynb)\n",
    "\n",
    "This notebook is available as a Jupyter notebook — clone the repo and run it yourself!\n",
    "\n",
    "**What This Notebook Demonstrates**\n",
    "\n",
    "This is a **complete, production-ready example** of evaluating multi-agent systems using the MASEval library. By the end, you'll understand how to:\n",
    "\n",
    "1. **Build multi-agent systems** with orchestrators and specialists\n",
    "2. **Create framework-agnostic tools** that work across agent libraries  \n",
    "3. **Organize systematic evaluation** using Tasks, Environments, and Benchmarks\n",
    "4. **Implement custom evaluators** (unit tests, LLM judges, pattern matching)\n",
    "5. **Run reproducible benchmarks** with automatic tool tracing\n",
    "\n",
    "\n",
    "**Prerequisites**: Familiarity with LLM agents and basic Python programming.\n",
    "\n",
    "## The 5-A-Day Benchmark\n",
    "\n",
    "We implement 5 diverse tasks representing real-world agent scenarios:\n",
    "\n",
    "| Task | Scenario | Tools | Evaluation |\n",
    "|------|----------|-------|------------|\n",
    "| 0 | Email & Banking | email, banking | LLM judge, pattern matching |\n",
    "| 1 | Finance Calculation | stock_price, calculator, family_info | Arithmetic verification |\n",
    "| 2 | Code Generation | python_executor | Unit tests, complexity analysis |\n",
    "| 3 | Calendar Scheduling | calendar | Slot matching, logic validation |\n",
    "| 4 | Hotel Optimization | hotel_search | Ranking, search strategy |\n",
    "\n",
    "We'll use a **multi-agent architecture**: an orchestrator delegates to specialized agents (banking specialist, email specialist, etc.), demonstrating MASEval's support for complex agent systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61559a7",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Multi-Agent Systems\n",
    "\n",
    "Before diving into MASEval, let's build the multi-agent system we'll be evaluating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f814fa",
   "metadata": {},
   "source": [
    "### 1.1 Imports and Setup\n",
    "\n",
    "We use **smolagents** as our agent framework (MASEval also supports LangGraph and LlamaIndex).\n",
    "\n",
    "The imports below include:\n",
    "- **Standard libraries**: `json`, `os`, `Path` for file handling\n",
    "- **Helper utilities**: Functions from this example's `utils.py` and `tools.py`  \n",
    "- **smolagents**: The agent framework we'll use\n",
    "- **MASEval core**: The evaluation orchestration library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954a7569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Sequence\n",
    "\n",
    "# Utility functions from this example\n",
    "# - derive_seed(): Creates reproducible seeds from task_id + agent_id\n",
    "# - sanitize_name(): Cleans agent names for framework compatibility\n",
    "from utils import derive_seed, sanitize_name\n",
    "\n",
    "# Tool collection classes and helpers\n",
    "# - EmailToolCollection, BankingToolCollection: Pre-built tool groups\n",
    "# - filter_tool_adapters_by_prefix(): Selects tools by name prefix\n",
    "# - get_states(): Initializes tool state objects (email inboxes, bank accounts, etc.)\n",
    "from tools import (\n",
    "    EmailToolCollection,\n",
    "    BankingToolCollection,\n",
    "    CalculatorToolCollection,\n",
    "    CodeExecutionToolCollection,\n",
    "    FamilyInfoToolCollection,\n",
    "    StockPriceToolCollection,\n",
    "    CalendarToolCollection,\n",
    "    HotelSearchToolCollection,\n",
    "    filter_tool_adapters_by_prefix,\n",
    "    get_states,\n",
    ")\n",
    "\n",
    "# smolagents: Our chosen agent framework\n",
    "from smolagents import ToolCallingAgent, LiteLLMModel, FinalAnswerTool\n",
    "\n",
    "# MASEval core components\n",
    "from maseval import Benchmark, Environment, Task, TaskCollection, AgentAdapter, Evaluator\n",
    "from maseval.interface.agents.smolagents import SmolAgentAdapter\n",
    "from maseval.core.callbacks.result_logger import FileResultLogger\n",
    "\n",
    "# Import evaluators module (dynamically loaded later)\n",
    "import evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd93371",
   "metadata": {},
   "source": [
    "### 1.2 Model Factory\n",
    "\n",
    "We need LLMs to power our agents. This factory function creates models using LiteLLM, which provides a unified interface to many providers (OpenAI, Anthropic, Google, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bd1476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_id: str, temperature: float = 0.7, seed: int | None = None):\n",
    "    \"\"\"Create a model instance compatible with smolagents.\n",
    "    \n",
    "    Args:\n",
    "        model_id: Model name (e.g., 'gemini-2.5-flash', 'gpt-4')\n",
    "        temperature: Randomness (0.0 = deterministic, 1.0 = creative)\n",
    "        seed: Random seed for reproducible outputs\n",
    "    \n",
    "    Returns:\n",
    "        LiteLLMModel configured for smolagents\n",
    "    \"\"\"\n",
    "    return LiteLLMModel(\n",
    "        model_id=f\"gemini/{model_id}\",  # Prefix determines provider\n",
    "        api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "        temperature=temperature,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "# Test the model factory\n",
    "model = get_model(\"gemini-2.5-flash\", temperature=0.7, seed=42)\n",
    "print(f\"✓ Created model: {model.model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9bd804",
   "metadata": {},
   "source": [
    "### 1.3 Loading Task Data\n",
    "\n",
    "Tasks are defined in JSON files. Let's examine one to understand the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d869e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load task and multi-agent configuration data\n",
    "data_dir = Path(\"data\")\n",
    "\n",
    "with open(data_dir / \"tasks.json\", \"r\") as f:\n",
    "    tasks_raw = json.load(f)\n",
    "\n",
    "with open(data_dir / \"multiagent.json\", \"r\") as f:  # Note: using multi-agent configs\n",
    "    configs_raw = json.load(f)\n",
    "\n",
    "# Examine Task 0: Email & Banking\n",
    "task_0 = tasks_raw[0]\n",
    "config_0 = configs_raw[0]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TASK 0: Email & Banking\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nUser Query:\\n{task_0['query']}\\n\")\n",
    "print(f\"Required Tools: {task_0['environment_data']['tools']}\")\n",
    "print(f\"\\nEvaluators: {task_0['evaluation_data']['evaluators']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68030cf",
   "metadata": {},
   "source": [
    "#### Multi-Agent Configuration\n",
    "\n",
    "For this task, we use **3 agents**:\n",
    "1. **Orchestrator** - Coordinates specialists  \n",
    "2. **Banking Specialist** - Handles financial data\n",
    "3. **Email Specialist** - Manages email operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4067ec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Multi-Agent Setup:\")\n",
    "print(f\"Agent Type: {config_0['agent_type']}\")\n",
    "print(f\"Primary Agent: {config_0['primary_agent_id']}\\n\")\n",
    "\n",
    "for i, agent_spec in enumerate(config_0['agents'], 1):\n",
    "    print(f\"{i}. {agent_spec['agent_name']} (ID: {agent_spec['agent_id']})\")\n",
    "    print(f\"   Tools: {agent_spec['tools'] if agent_spec['tools'] else 'None (delegates only)'}\")\n",
    "    print(f\"   Role: {agent_spec['agent_instruction'][:80]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14523c4",
   "metadata": {},
   "source": [
    "### 1.4 Creating Tools for Specialist Agents\n",
    "\n",
    "Tools are functions agents can call. We'll create email and banking tools for our specialists.\n",
    "\n",
    "**Key insight**: Our tools are \"framework-agnostic\" `BaseTool` objects that convert to any framework (smolagents, LangGraph, LlamaIndex)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d903b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize state objects from task data\n",
    "# These hold the actual data (emails, bank transactions) that tools operate on\n",
    "env_data = task_0[\"environment_data\"].copy()\n",
    "states = get_states(env_data[\"tools\"], env_data)\n",
    "\n",
    "# Create tool collections\n",
    "email_tools = EmailToolCollection(states[\"email_state\"])\n",
    "banking_tools = BankingToolCollection(states[\"banking_state\"])\n",
    "\n",
    "# Convert to smolagents format (returns tool adapters with tracing support)\n",
    "email_adapters = [tool.to_smolagents() for tool in email_tools.get_sub_tools()]\n",
    "banking_adapters = [tool.to_smolagents() for tool in banking_tools.get_sub_tools()]\n",
    "\n",
    "# Extract raw smolagents Tool objects\n",
    "all_tool_adapters = email_adapters + banking_adapters\n",
    "all_tools = [adapter.tool for adapter in all_tool_adapters]\n",
    "\n",
    "print(f\"✓ Created {len(all_tools)} tools:\")\n",
    "for tool in all_tools:\n",
    "    print(f\"  - {tool.name}: {tool.description[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9d793d",
   "metadata": {},
   "source": [
    "### 1.5 Building the Multi-Agent System\n",
    "\n",
    "Now we build our 3 agents:\n",
    "- **Specialist agents** get tools + `FinalAnswerTool()` (to return results)\n",
    "- **Orchestrator** gets specialists as `managed_agents` (can delegate to them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0eb754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build specialist agents\n",
    "specialist_agents = []\n",
    "\n",
    "for agent_spec in config_0['agents']:\n",
    "    if agent_spec['agent_id'] == config_0['primary_agent_id']:\n",
    "        continue  # Skip orchestrator for now\n",
    "    \n",
    "    # Get tools for this specialist\n",
    "    spec_tool_adapters = filter_tool_adapters_by_prefix(all_tool_adapters, agent_spec['tools'])\n",
    "    spec_tools = [adapter.tool for adapter in spec_tool_adapters]\n",
    "    spec_tools.append(FinalAnswerTool())  # Specialists need this to return results\n",
    "    \n",
    "    # Create specialist agent\n",
    "    specialist = ToolCallingAgent(\n",
    "        model=get_model(\"gemini-2.5-flash\", seed=42),\n",
    "        tools=spec_tools,\n",
    "        name=sanitize_name(agent_spec['agent_name']),\n",
    "        description=agent_spec['agent_instruction'],\n",
    "        instructions=agent_spec['agent_instruction'],\n",
    "        verbosity_level=2,\n",
    "    )\n",
    "    specialist_agents.append(specialist)\n",
    "    print(f\"✓ Created {agent_spec['agent_name']} with {len(spec_tools)} tools\")\n",
    "\n",
    "# Build orchestrator with managed agents\n",
    "orchestrator_spec = next(a for a in config_0['agents'] if a['agent_id'] == config_0['primary_agent_id'])\n",
    "\n",
    "orchestrator = ToolCallingAgent(\n",
    "    model=get_model(\"gemini-2.5-flash\", seed=42),\n",
    "    tools=[FinalAnswerTool()],  # Orchestrator has no direct tools\n",
    "    managed_agents=specialist_agents,  # But can delegate!\n",
    "    name=sanitize_name(orchestrator_spec['agent_name']),\n",
    "    instructions=orchestrator_spec['agent_instruction'],\n",
    "    verbosity_level=2,\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Created {orchestrator_spec['agent_name']} managing {len(specialist_agents)} specialists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79568554",
   "metadata": {},
   "source": [
    "## Part 2: Organizing Evaluation with MASEval\n",
    "\n",
    "We've built a multi-agent system. Now let's see how **MASEval** helps us evaluate it systematically across multiple tasks.\n",
    "\n",
    "MASEval provides three key abstractions:\n",
    "\n",
    "1. **Task** - A single evaluation scenario (query + environment + evaluation criteria)\n",
    "2. **Environment** - Manages tools and state for a task\n",
    "3. **Benchmark** - Orchestrates running agents on tasks and collecting results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dc0505",
   "metadata": {},
   "source": [
    "### 2.1 The Environment Class\n",
    "\n",
    "An `Environment` creates and manages tools for each task. It also enables automatic tool tracing.\n",
    "\n",
    "**Key methods**:\n",
    "- `setup_state()`: Initialize tool state (email inboxes, bank accounts, etc.)\n",
    "- `create_tools()`: Create and convert tools to framework-specific format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d95d447",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiveADayEnvironment(Environment):\n",
    "    \"\"\"Environment that creates framework-specific tools from task data.\"\"\"\n",
    "\n",
    "    def __init__(self, task_data: Dict[str, Any], framework: str, callbacks: List | None = None):\n",
    "        self.framework = framework\n",
    "        super().__init__(task_data, callbacks)\n",
    "\n",
    "    def setup_state(self, task_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Initialize environment state from task data.\"\"\"\n",
    "        env_data = task_data[\"environment_data\"].copy()\n",
    "        tool_names = env_data.get(\"tools\", [])\n",
    "        \n",
    "        # Create state objects (e.g., email inboxes, bank accounts)\n",
    "        states = get_states(tool_names, env_data)\n",
    "        env_data.update(states)\n",
    "        \n",
    "        return env_data\n",
    "\n",
    "    def create_tools(self) -> list:\n",
    "        \"\"\"Create and convert tools to framework-specific format.\"\"\"\n",
    "        tools_list = []\n",
    "        \n",
    "        # Map tool names to their collection classes\n",
    "        tool_mapping = {\n",
    "            \"email\": (EmailToolCollection, lambda: (self.state[\"email_state\"],)),\n",
    "            \"banking\": (BankingToolCollection, lambda: (self.state[\"banking_state\"],)),\n",
    "            \"calculator\": (CalculatorToolCollection, lambda: ()),\n",
    "            \"python_executor\": (CodeExecutionToolCollection, lambda: (self.state[\"python_executor_state\"],)),\n",
    "            \"family_info\": (FamilyInfoToolCollection, lambda: (self.state[\"family_info\"],)),\n",
    "            \"stock_price\": (StockPriceToolCollection, lambda: (self.state[\"stock_price_lookup\"],)),\n",
    "            \"calendar\": (CalendarToolCollection, lambda: (self.state[\"calendar_state\"],)),\n",
    "            \"hotel_search\": (HotelSearchToolCollection, lambda: (self.state[\"hotel_search_state\"],)),\n",
    "        }\n",
    "        \n",
    "        for tool_name in self.state[\"tools\"]:\n",
    "            if tool_name in tool_mapping:\n",
    "                ToolClass, get_init_args = tool_mapping[tool_name]\n",
    "                tool_instance = ToolClass(*get_init_args())\n",
    "                \n",
    "                # Get base tools and convert to framework format\n",
    "                for base_tool in tool_instance.get_sub_tools():\n",
    "                    if self.framework == \"smolagents\":\n",
    "                        framework_tool = base_tool.to_smolagents()\n",
    "                    # ... other frameworks supported\n",
    "                    tools_list.append(framework_tool)\n",
    "        \n",
    "        return tools_list\n",
    "\n",
    "print(\"✓ FiveADayEnvironment defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565199e8",
   "metadata": {},
   "source": [
    "### 2.2 The Benchmark Class\n",
    "\n",
    "A `Benchmark` orchestrates the entire evaluation process. It implements 5 key methods:\n",
    "\n",
    "1. **setup_environment()** - Create tools for a task\n",
    "2. **setup_agents()** - Build agents with appropriate tools\n",
    "3. **setup_evaluators()** - Create task-specific evaluators\n",
    "4. **run_agents()** - Execute agents and collect responses\n",
    "5. **evaluate()** - Run evaluators on agent outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c66cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiveADayBenchmark(Benchmark):\n",
    "    \"\"\"5-A-Day benchmark with multi-agent support.\"\"\"\n",
    "\n",
    "    def setup_environment(self, agent_data: Dict[str, Any], task: Task) -> Environment:\n",
    "        \"\"\"Create environment from task data.\"\"\"\n",
    "        task_data = {\n",
    "            \"environment_data\": task.environment_data,\n",
    "            \"query\": task.query,\n",
    "            \"evaluation_data\": task.evaluation_data,\n",
    "            \"metadata\": task.metadata,\n",
    "        }\n",
    "        \n",
    "        framework = agent_data[\"framework\"]\n",
    "        environment = FiveADayEnvironment(task_data, framework)\n",
    "        \n",
    "        # Register all tools for tracing\n",
    "        for tool_adapter in environment.get_tools():\n",
    "            tool_name = getattr(tool_adapter, \"name\", str(type(tool_adapter).__name__))\n",
    "            self.register(\"tools\", tool_name, tool_adapter)\n",
    "        \n",
    "        return environment\n",
    "\n",
    "    def setup_agents(\n",
    "        self, agent_data: Dict[str, Any], environment: Environment, task: Task, user=None\n",
    "    ) -> tuple[list[AgentAdapter], Dict[str, AgentAdapter]]:\n",
    "        \"\"\"Create multi-agent system with orchestrator and specialists.\"\"\"\n",
    "        framework = agent_data[\"framework\"]\n",
    "        model_id = agent_data[\"model_config\"][\"model_id\"]\n",
    "        temperature = agent_data[\"model_config\"][\"temperature\"]\n",
    "        \n",
    "        primary_agent_id = agent_data[\"primary_agent_id\"]\n",
    "        agents_specs = agent_data[\"agents\"]\n",
    "        all_tool_adapters = environment.get_tools()\n",
    "        \n",
    "        # Build specialists first\n",
    "        specialist_agents = []\n",
    "        for agent_spec in agents_specs:\n",
    "            if agent_spec[\"agent_id\"] == primary_agent_id:\n",
    "                continue\n",
    "                \n",
    "            seed = agent_spec.get(\"seed\")\n",
    "            model = get_model(model_id, temperature, seed)\n",
    "            spec_tool_adapters = filter_tool_adapters_by_prefix(all_tool_adapters, agent_spec[\"tools\"])\n",
    "            spec_tools = [adapter.tool for adapter in spec_tool_adapters]\n",
    "            spec_tools.append(FinalAnswerTool())\n",
    "            \n",
    "            specialist = ToolCallingAgent(\n",
    "                model=model,\n",
    "                tools=spec_tools,\n",
    "                name=sanitize_name(agent_spec[\"agent_name\"]),\n",
    "                description=agent_spec[\"agent_instruction\"],\n",
    "                instructions=agent_spec[\"agent_instruction\"],\n",
    "                verbosity_level=2,\n",
    "            )\n",
    "            specialist_agents.append(specialist)\n",
    "        \n",
    "        # Build orchestrator\n",
    "        primary_spec = next(a for a in agents_specs if a[\"agent_id\"] == primary_agent_id)\n",
    "        primary_seed = primary_spec.get(\"seed\")\n",
    "        primary_model = get_model(model_id, temperature, primary_seed)\n",
    "        \n",
    "        orchestrator = ToolCallingAgent(\n",
    "            model=primary_model,\n",
    "            tools=[FinalAnswerTool()],\n",
    "            managed_agents=specialist_agents if specialist_agents else None,\n",
    "            name=sanitize_name(primary_spec[\"agent_name\"]),\n",
    "            instructions=primary_spec[\"agent_instruction\"],\n",
    "            verbosity_level=2,\n",
    "        )\n",
    "        \n",
    "        agent_adapter = SmolAgentAdapter(orchestrator, primary_spec[\"agent_id\"])\n",
    "        return [agent_adapter], {primary_agent_id: agent_adapter}\n",
    "\n",
    "    def setup_evaluators(self, environment, task, agents, user) -> Sequence[Evaluator]:\n",
    "        \"\"\"Create evaluators based on task's evaluation criteria.\"\"\"\n",
    "        if not task.evaluation_data[\"evaluators\"]:\n",
    "            return []\n",
    "        \n",
    "        evaluator_instances = []\n",
    "        for name in task.evaluation_data[\"evaluators\"]:\n",
    "            evaluator_class = getattr(evaluators, name)\n",
    "            evaluator_instances.append(evaluator_class(task, environment, user))\n",
    "        \n",
    "        return evaluator_instances\n",
    "\n",
    "    def run_agents(self, agents: Sequence[AgentAdapter], task: Task, environment: Environment) -> Sequence[Any]:\n",
    "        \"\"\"Execute agents and return their final answers.\"\"\"\n",
    "        answers = [agent.run(task.query) for agent in agents]\n",
    "        return answers\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        evaluators: Sequence[Evaluator],\n",
    "        agents: Dict[str, AgentAdapter],\n",
    "        final_answer: Any,\n",
    "        traces: Dict[str, Any],\n",
    "    ) -> list[Dict[str, Any]]:\n",
    "        \"\"\"Evaluate agent performance.\"\"\"\n",
    "        results = []\n",
    "        for evaluator in evaluators:\n",
    "            filtered_traces = evaluator.filter_traces(traces)\n",
    "            results.append(evaluator(filtered_traces, final_answer))\n",
    "        return results\n",
    "\n",
    "print(\"✓ FiveADayBenchmark defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cd22cb",
   "metadata": {},
   "source": [
    "### 2.3 Loading All Tasks\n",
    "\n",
    "Let's create a helper function to load all 5 tasks with their multi-agent configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04bbd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_benchmark_data(\n",
    "    config_type: str = \"multi\",\n",
    "    framework: str = \"smolagents\",\n",
    "    model_id: str = \"gemini-2.5-flash\",\n",
    "    temperature: float = 0.7,\n",
    "    limit: int | None = None,\n",
    "    seed: int | None = None,\n",
    ") -> tuple[TaskCollection, list[Dict[str, Any]]]:\n",
    "    \"\"\"Load tasks and agent configurations.\n",
    "    \n",
    "    Args:\n",
    "        config_type: 'single' or 'multi' agent configuration\n",
    "        framework: Agent framework to use\n",
    "        model_id: Model identifier\n",
    "        temperature: Model temperature\n",
    "        limit: Optional limit on number of tasks (None = all 5)\n",
    "        seed: Random seed for reproducibility\n",
    "    \"\"\"\n",
    "    data_dir = Path(\"data\")\n",
    "    \n",
    "    with open(data_dir / \"tasks.json\", \"r\") as f:\n",
    "        tasks_raw = json.load(f)\n",
    "    with open(data_dir / f\"{config_type}agent.json\", \"r\") as f:\n",
    "        configs_raw = json.load(f)\n",
    "    \n",
    "    if limit:\n",
    "        tasks_raw = tasks_raw[:limit]\n",
    "        configs_raw = configs_raw[:limit]\n",
    "    \n",
    "    tasks_data = []\n",
    "    configs_data = []\n",
    "    \n",
    "    for task_dict, config in zip(tasks_raw, configs_raw):\n",
    "        task_id = task_dict[\"metadata\"][\"task_id\"]\n",
    "        task_dict[\"environment_data\"][\"agent_framework\"] = framework\n",
    "        \n",
    "        # Create Task object\n",
    "        tasks_data.append(\n",
    "            Task(\n",
    "                query=task_dict[\"query\"],\n",
    "                environment_data=task_dict[\"environment_data\"],\n",
    "                evaluation_data=task_dict[\"evaluation_data\"],\n",
    "                metadata=task_dict[\"metadata\"],\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Enrich config with framework and model info\n",
    "        config[\"framework\"] = framework\n",
    "        config[\"model_config\"] = {\"model_id\": model_id, \"temperature\": temperature}\n",
    "        \n",
    "        # Derive seeds for reproducibility\n",
    "        if seed is not None:\n",
    "            for agent_spec in config[\"agents\"]:\n",
    "                agent_spec[\"seed\"] = derive_seed(seed, task_id, agent_spec[\"agent_id\"])\n",
    "        \n",
    "        configs_data.append(config)\n",
    "    \n",
    "    return TaskCollection(tasks_data), configs_data\n",
    "\n",
    "# Load all 5 tasks\n",
    "tasks, agent_configs = load_benchmark_data(\n",
    "    config_type=\"multi\",\n",
    "    framework=\"smolagents\",\n",
    "    model_id=\"gemini-2.5-flash\",\n",
    "    temperature=0.7,\n",
    "    limit=None,  # Set to 1-2 for quick testing\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"✓ Loaded {len(tasks)} tasks:\")\n",
    "for i, task in enumerate(tasks):\n",
    "    print(f\"  {i}. {task.metadata['task_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1aa095f",
   "metadata": {},
   "source": [
    "### 2.4 Running the Benchmark\n",
    "\n",
    "Now we can run the complete benchmark! MASEval will:\n",
    "1. Create environments for each task\n",
    "2. Build multi-agent systems with appropriate tools\n",
    "3. Run agents and collect traces (tool calls, messages, etc.)\n",
    "4. Evaluate results using task-specific evaluators\n",
    "5. Log everything to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3764c0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a logger to save results\n",
    "logger = FileResultLogger(\n",
    "    output_dir=\"results\",\n",
    "    filename_pattern=\"smolagents_multi_{timestamp}.jsonl\",\n",
    "    validate_on_completion=False,\n",
    ")\n",
    "\n",
    "# Create and run benchmark\n",
    "benchmark = FiveADayBenchmark(\n",
    "    agent_data=agent_configs,\n",
    "    callbacks=[logger],\n",
    "    fail_on_setup_error=True,\n",
    "    fail_on_task_error=True,\n",
    "    fail_on_evaluation_error=True,\n",
    ")\n",
    "\n",
    "print(\"Running benchmark...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = benchmark.run(tasks=tasks)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"✓ Benchmark complete!\")\n",
    "print(f\"✓ Ran {len(tasks)} tasks\")\n",
    "print(f\"✓ Results saved to: {logger.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fb97da",
   "metadata": {},
   "source": [
    "### 2.5 Examining Results\n",
    "\n",
    "Let's look at the results for one task to understand the output structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61559df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine Task 0 results\n",
    "task_0_result = results[0]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TASK 0 RESULTS: Email & Banking\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nTask: {task_0_result['task']['metadata']['task_name']}\")\n",
    "print(f\"Query: {task_0_result['task']['query'][:100]}...\")\n",
    "\n",
    "print(f\"\\n{'Final Answer:':-^60}\")\n",
    "print(task_0_result['final_answer'][:300] + \"...\")\n",
    "\n",
    "print(f\"\\n{'Evaluation Results:':-^60}\")\n",
    "for eval_result in task_0_result[\"evaluation_results\"]:\n",
    "    print(f\"\\n{eval_result['evaluator_name']}:\")\n",
    "    if \"score\" in eval_result:\n",
    "        print(f\"  Score: {eval_result['score']}\")\n",
    "    if \"passed\" in eval_result:\n",
    "        print(f\"  Passed: {eval_result['passed']}\")\n",
    "    if \"details\" in eval_result:\n",
    "        print(f\"  Details: {str(eval_result['details'])[:100]}...\")\n",
    "\n",
    "print(f\"\\n{'Tool Usage:':-^60}\")\n",
    "if \"traces\" in task_0_result and \"tools\" in task_0_result[\"traces\"]:\n",
    "    for tool_name, tool_trace in task_0_result[\"traces\"][\"tools\"].items():\n",
    "        print(f\"  {tool_name}: {len(tool_trace)} call(s)\")\n",
    "else:\n",
    "    print(\"  No tool traces available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e682b9",
   "metadata": {},
   "source": [
    "### 2.6 Summary Statistics\n",
    "\n",
    "Let's compute overall performance across all tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606515af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute summary statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"BENCHMARK SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total_tasks = len(results)\n",
    "total_evaluations = sum(len(r[\"evaluation_results\"]) for r in results)\n",
    "\n",
    "print(f\"\\nTotal tasks: {total_tasks}\")\n",
    "print(f\"Total evaluations: {total_evaluations}\")\n",
    "\n",
    "# Count passed evaluations\n",
    "passed_count = 0\n",
    "total_count = 0\n",
    "for result in results:\n",
    "    for eval_result in result[\"evaluation_results\"]:\n",
    "        if \"passed\" in eval_result:\n",
    "            total_count += 1\n",
    "            if eval_result[\"passed\"]:\n",
    "                passed_count += 1\n",
    "\n",
    "if total_count > 0:\n",
    "    print(f\"Pass rate: {passed_count}/{total_count} ({100*passed_count/total_count:.1f}%)\")\n",
    "\n",
    "# Tool usage summary\n",
    "all_tools_used = set()\n",
    "for result in results:\n",
    "    if \"traces\" in result and \"tools\" in result[\"traces\"]:\n",
    "        all_tools_used.update(result[\"traces\"][\"tools\"].keys())\n",
    "\n",
    "print(f\"\\nUnique tools used: {len(all_tools_used)}\")\n",
    "print(f\"Tools: {', '.join(sorted(all_tools_used))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080f6216",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "You now understand how to build production agent benchmarks with MASEval:\n",
    "\n",
    "#### Part 1: Multi-Agent Systems\n",
    "- **Model creation** with LiteLLM for framework compatibility\n",
    "- **Framework-agnostic tools** that convert to any agent library\n",
    "- **Multi-agent architecture** with orchestrators and specialists\n",
    "- **Tool state management** for realistic task environments\n",
    "\n",
    "#### Part 2: MASEval Framework\n",
    "- **Task abstraction** packages queries, environments, and evaluation criteria\n",
    "- **Environment class** creates tools and enables automatic tracing\n",
    "- **Benchmark class** orchestrates evaluation across multiple tasks\n",
    "- **Custom evaluators** for diverse evaluation approaches (unit tests, LLM judges, etc.)\n",
    "- **Automatic tracing** captures all tool calls and agent interactions\n",
    "\n",
    "### Key Design Patterns\n",
    "\n",
    "1. **Separation of Concerns**:\n",
    "   - Tasks define WHAT to evaluate\n",
    "   - Environments provides a world in which the agents act (tools and state)\n",
    "   - Benchmarks orchestrate WHEN and WHERE\n",
    "   - Evaluators determine SUCCESS\n",
    "\n",
    "2. **Framework Agnostic**:\n",
    "   - Same tasks work with smolagents, LangGraph, LlamaIndex\n",
    "   - Tools convert automatically to framework-specific formats\n",
    "   - Easy to compare frameworks on identical tasks\n",
    "\n",
    "3. **Reproducibility**:\n",
    "   - Seeds derived systematically from task_id + agent_id\n",
    "   - All parameters logged automatically\n",
    "   - Results saved in structured JSONL format\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Explore evaluators** — Check `evaluators/` for different evaluation strategies\n",
    "2. **Try single-agent mode** — Load `data/singleagent.json` to compare architectures\n",
    "3. **Run from CLI** — Use `five_a_day_benchmark.py` for scripted runs with different frameworks\n",
    "4. **Add custom tasks** — Create your own task definitions and evaluators\n",
    "5. **Compare frameworks** — Run the same benchmark with LangGraph or LlamaIndex\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [MASEval Documentation](https://github.com/parameterlab/MASEval)\n",
    "- Example code: [`examples/five_a_day_benchmark/`](https://github.com/parameterlab/MASEval/tree/main/examples/five_a_day_benchmark)\n",
    "- Example data: [`examples/five_a_day_benchmark/data/`]((https://github.com/parameterlab/MASEval/tree/main/examples/five_a_day_benchmark/data)\n",
    "- Tool implementations: [`examples/five_a_day_benchmark/tools/`]((https://github.com/parameterlab/MASEval/tree/main/examples/five_a_day_benchmark/tools)\n",
    "- Evaluator implementations: [`examples/five_a_day_benchmark/evaluators/`((https://github.com/parameterlab/MASEval/tree/main/examples/five_a_day_benchmark/evaluators)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
