{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fd80dc4",
   "metadata": {},
   "source": [
    "# 5 A Day Benchmark\n",
    "\n",
    "[![Open Notebook on GitHub](https://img.shields.io/badge/Open%20Notebook%20on-GitHub-blue?logo=github)](https://github.com/parameterlab/MASEval/blob/main/examples/five_a_day_benchmark/five_a_day_benchmark.ipynb)\n",
    "\n",
    "This notebook is available as a Jupyter notebook — clone the repo and run it yourself!\n",
    "\n",
    "**What This Notebook Demonstrates**\n",
    "\n",
    "This is a **complete, production-ready example** of evaluating multi-agent systems using the MASEval library. By the end, you'll understand how to:\n",
    "\n",
    "1. **Build multi-agent systems** with orchestrators and specialists\n",
    "2. **Create framework-agnostic tools** that work across agent libraries  \n",
    "3. **Organize systematic evaluation** using Tasks, Environments, and Benchmarks\n",
    "4. **Implement custom evaluators** (unit tests, LLM judges, pattern matching)\n",
    "5. **Run reproducible benchmarks** with automatic tool tracing\n",
    "\n",
    "\n",
    "**Prerequisites**: Familiarity with LLM agents and basic Python programming.\n",
    "\n",
    "## The 5-A-Day Benchmark\n",
    "\n",
    "We implement 5 diverse tasks representing real-world agent scenarios:\n",
    "\n",
    "| Task | Scenario | Tools | Evaluation |\n",
    "|------|----------|-------|------------|\n",
    "| 0 | Email & Banking | email, banking | LLM judge, pattern matching |\n",
    "| 1 | Finance Calculation | stock_price, calculator, family_info | Arithmetic verification |\n",
    "| 2 | Code Generation | python_executor | Unit tests, complexity analysis |\n",
    "| 3 | Calendar Scheduling | calendar | Slot matching, logic validation |\n",
    "| 4 | Hotel Optimization | hotel_search | Ranking, search strategy |\n",
    "\n",
    "We'll use a **multi-agent architecture**: an orchestrator delegates to specialized agents (banking specialist, email specialist, etc.), demonstrating MASEval's support for complex agent systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61559a7",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Multi-Agent Systems\n",
    "\n",
    "Before diving into MASEval, let's build the multi-agent system we'll be evaluating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f814fa",
   "metadata": {},
   "source": [
    "### 1.1 Imports and Setup\n",
    "\n",
    "We use **smolagents** as our agent framework (MASEval also supports LangGraph and LlamaIndex).\n",
    "\n",
    "The imports below include:\n",
    "- **Standard libraries**: `json`, `os`, `Path` for file handling\n",
    "- **Helper utilities**: Functions from this example's `utils.py` and `tools.py`  \n",
    "- **smolagents**: The agent framework we'll use\n",
    "- **MASEval core**: The evaluation orchestration library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954a7569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa E402\n",
    "# Setup: Set working directory to project root for proper imports\n",
    "# This must happen FIRST before any other imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Any, Dict, List, Sequence\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "\n",
    "# Determine notebook directory and set working directory to project root\n",
    "_notebook_dir = Path(__file__).parent if \"__file__\" in dir() else Path.cwd()\n",
    "if _notebook_dir.name == \"five_a_day_benchmark\":\n",
    "    _project_root = _notebook_dir.parent.parent\n",
    "    os.chdir(_project_root)\n",
    "    # Add project root to path so `examples.five_a_day_benchmark.*` imports work\n",
    "    if str(_project_root) not in sys.path:\n",
    "        sys.path.insert(0, str(_project_root))\n",
    "    # Also add the example directory for local imports (utils, tools, evaluators)\n",
    "    if str(_notebook_dir) not in sys.path:\n",
    "        sys.path.insert(0, str(_notebook_dir))\n",
    "    print(f\"Working directory set to: {os.getcwd()}\")\n",
    "\n",
    "\n",
    "# Utility functions from this example\n",
    "# - derive_seed(): Creates reproducible seeds from task_id + agent_id\n",
    "# - sanitize_name(): Cleans agent names for framework compatibility\n",
    "from utils import derive_seed, sanitize_name\n",
    "\n",
    "# Tool collection classes and helpers\n",
    "# - EmailToolCollection, BankingToolCollection: Pre-built tool groups\n",
    "# - filter_tool_adapters_by_prefix(): Selects tools by name prefix\n",
    "# - get_states(): Initializes tool state objects (email inboxes, bank accounts, etc.)\n",
    "from tools import (\n",
    "    EmailToolCollection,\n",
    "    BankingToolCollection,\n",
    "    CalculatorToolCollection,\n",
    "    CodeExecutionToolCollection,\n",
    "    FamilyInfoToolCollection,\n",
    "    StockPriceToolCollection,\n",
    "    CalendarToolCollection,\n",
    "    HotelSearchToolCollection,\n",
    "    MCPCalendarToolCollection,\n",
    "    filter_tool_adapters_by_prefix,\n",
    "    get_states,\n",
    ")\n",
    "\n",
    "# smolagents: Our chosen agent framework\n",
    "from smolagents import ToolCallingAgent, LiteLLMModel, FinalAnswerTool\n",
    "\n",
    "# MASEval core components\n",
    "from maseval import Benchmark, Environment, Task, TaskQueue, AgentAdapter, Evaluator, ModelAdapter\n",
    "from maseval.interface.agents.smolagents import SmolAgentAdapter\n",
    "\n",
    "# Import evaluators module (dynamically loaded later)\n",
    "import evaluators\n",
    "\n",
    "\n",
    "def load_benchmark_data(\n",
    "    config_type: str = \"multi\",\n",
    "    framework: str = \"smolagents\",\n",
    "    model_id: str = \"gemini-2.5-flash\",\n",
    "    temperature: float = 0.7,\n",
    "    limit: int | None = None,\n",
    "    seed: int | None = None,\n",
    "    task_indices: list[int] | None = None,\n",
    ") -> tuple[TaskQueue, list[Dict[str, Any]]]:\n",
    "    \"\"\"Load tasks and agent configurations.\n",
    "\n",
    "    Args:\n",
    "        config_type: 'single' or 'multi' agent configuration\n",
    "        framework: Agent framework to use\n",
    "        model_id: Model identifier\n",
    "        temperature: Model temperature\n",
    "        limit: Optional limit on number of tasks (None = all 5)\n",
    "        seed: Random seed for reproducibility\n",
    "        task_indices: Optional list of task indices to load (e.g., [0, 2, 4])\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (TaskQueue, list of agent configs)\n",
    "    \"\"\"\n",
    "    data_dir = Path(\"examples/five_a_day_benchmark/data\")\n",
    "\n",
    "    with open(data_dir / \"tasks.json\", \"r\") as f:\n",
    "        tasks_raw = json.load(f)\n",
    "    with open(data_dir / f\"{config_type}agent.json\", \"r\") as f:\n",
    "        configs_raw = json.load(f)\n",
    "\n",
    "    # Apply limit first\n",
    "    if limit:\n",
    "        tasks_raw = tasks_raw[:limit]\n",
    "        configs_raw = configs_raw[:limit]\n",
    "\n",
    "    # Then apply task_indices filter if specified\n",
    "    if task_indices is not None:\n",
    "        tasks_raw = [tasks_raw[i] for i in task_indices if i < len(tasks_raw)]\n",
    "        configs_raw = [configs_raw[i] for i in task_indices if i < len(configs_raw)]\n",
    "\n",
    "    tasks_data = []\n",
    "    configs_data = []\n",
    "\n",
    "    for task_dict, config in zip(tasks_raw, configs_raw):\n",
    "        task_id = task_dict[\"metadata\"][\"task_id\"]\n",
    "        task_dict[\"environment_data\"][\"agent_framework\"] = framework\n",
    "\n",
    "        # Create Task object\n",
    "        tasks_data.append(\n",
    "            Task(\n",
    "                query=task_dict[\"query\"],\n",
    "                environment_data=task_dict[\"environment_data\"],\n",
    "                evaluation_data=task_dict[\"evaluation_data\"],\n",
    "                metadata=task_dict[\"metadata\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Enrich config with framework and model info\n",
    "        config[\"framework\"] = framework\n",
    "        config[\"model_config\"] = {\"model_id\": model_id, \"temperature\": temperature}\n",
    "\n",
    "        # Derive seeds for reproducibility\n",
    "        if seed is not None:\n",
    "            for agent_spec in config[\"agents\"]:\n",
    "                agent_spec[\"seed\"] = derive_seed(seed, task_id, agent_spec[\"agent_id\"])\n",
    "\n",
    "        configs_data.append(config)\n",
    "\n",
    "    return TaskQueue(tasks_data), configs_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd93371",
   "metadata": {},
   "source": [
    "### 1.2 Model Factory\n",
    "\n",
    "We need LLMs to power our agents. This factory function creates models using LiteLLM, which provides a unified interface to many providers (OpenAI, Anthropic, Google, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bd1476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "\n",
    "# Tell litellm to drop unsupported params (like 'seed' for Gemini)\n",
    "litellm.drop_params = True\n",
    "\n",
    "\n",
    "def get_model(model_id: str, temperature: float = 0.7, seed: int | None = None):\n",
    "    \"\"\"Create a model instance compatible with smolagents.\n",
    "\n",
    "    Args:\n",
    "        model_id: Model name (e.g., 'gemini-2.5-flash', 'gpt-4')\n",
    "        temperature: Randomness (0.0 = deterministic, 1.0 = creative)\n",
    "        seed: Random seed for reproducible outputs (ignored for models that don't support it)\n",
    "\n",
    "    Returns:\n",
    "        LiteLLMModel configured for smolagents\n",
    "    \"\"\"\n",
    "    return LiteLLMModel(\n",
    "        model_id=f\"gemini/{model_id}\",  # Prefix determines provider\n",
    "        api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "        temperature=temperature,\n",
    "        seed=seed,  # Will be dropped by litellm for providers that don't support it\n",
    "    )\n",
    "\n",
    "\n",
    "# Test the model factory\n",
    "model = get_model(\"gemini-2.5-flash\", temperature=0.7, seed=42)\n",
    "print(f\"Created model: {model.model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9bd804",
   "metadata": {},
   "source": [
    "### 1.3 Loading Task Data\n",
    "\n",
    "We use the `load_benchmark_data()` function to load tasks and agent configurations. Let's load Task 0 (Email & Banking) to examine its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d869e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Task 0 for demonstration in Part 1\n",
    "task_data, agent_configs = load_benchmark_data(\n",
    "    config_type=\"multi\",\n",
    "    framework=\"smolagents\",\n",
    "    model_id=\"gemini-2.5-flash\",\n",
    "    temperature=0.7,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Extract the first (and only) task and config\n",
    "task_0: Task = task_data[0]\n",
    "config_0: Dict[str, Any] = agent_configs[0]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TASK 0: Email & Banking\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nUser Query:\\n{task_0.query}\\n\")\n",
    "print(f\"Required Tools: {task_0.environment_data['tools']}\")\n",
    "print(f\"\\nEvaluators: {task_0.evaluation_data['evaluators']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68030cf",
   "metadata": {},
   "source": [
    "#### Multi-Agent Configuration\n",
    "\n",
    "For this task, we use **3 agents**:\n",
    "1. **Orchestrator** - Coordinates specialists  \n",
    "2. **Banking Specialist** - Handles financial data\n",
    "3. **Email Specialist** - Manages email operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4067ec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Multi-Agent Setup:\")\n",
    "print(f\"Agent Type: {config_0['agent_type']}\")\n",
    "print(f\"Primary Agent: {config_0['primary_agent_id']}\\n\")\n",
    "\n",
    "for i, agent_spec in enumerate(config_0[\"agents\"], 1):\n",
    "    print(f\"{i}. {agent_spec['agent_name']} (ID: {agent_spec['agent_id']})\")\n",
    "    print(f\"   Tools: {agent_spec['tools'] if agent_spec['tools'] else 'None (delegates only)'}\")\n",
    "    print(f\"   Role: {agent_spec['agent_instruction'][:80]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14523c4",
   "metadata": {},
   "source": [
    "### 1.4 Creating Tools for Specialist Agents\n",
    "\n",
    "Tools are functions agents can call. We'll create email and banking tools for our specialists.\n",
    "\n",
    "**Key insight**: Our tools are \"framework-agnostic\" `BaseTool` objects that convert to any framework (smolagents, LangGraph, LlamaIndex)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d903b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize state objects from task data\n",
    "# These hold the actual data (emails, bank transactions) that tools operate on\n",
    "env_data = task_0.environment_data.copy()\n",
    "states = get_states(env_data[\"tools\"], env_data)\n",
    "\n",
    "# Create tool collections (tools are examples)\n",
    "email_tools = EmailToolCollection(states[\"email_state\"])\n",
    "banking_tools = BankingToolCollection(states[\"banking_state\"])\n",
    "\n",
    "# Convert to smolagents format (returns tool adapters with tracing support)\n",
    "email_adapters = [tool.to_smolagents() for tool in email_tools.get_sub_tools()]\n",
    "banking_adapters = [tool.to_smolagents() for tool in banking_tools.get_sub_tools()]\n",
    "\n",
    "# Extract raw smolagents Tool objects\n",
    "all_tool_adapters = email_adapters + banking_adapters\n",
    "all_tools = [adapter.tool for adapter in all_tool_adapters]\n",
    "\n",
    "print(f\"Created {len(all_tools)} tools:\")\n",
    "for tool in all_tools:\n",
    "    print(f\"  - {tool.name}: {tool.description[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9d793d",
   "metadata": {},
   "source": [
    "### 1.5 Building the Multi-Agent System\n",
    "\n",
    "Now we build our 3 agents:\n",
    "- **Specialist agents** get tools + `FinalAnswerTool()` (to return results)\n",
    "- **Orchestrator** gets specialists as `managed_agents` (can delegate to them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0eb754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build specialist agents\n",
    "def build_agents(agent_data: Dict[str, Any], environment: Environment) -> tuple[list[ToolCallingAgent], Dict[str, ToolCallingAgent]]:\n",
    "    \"\"\"Create multi-agent system with orchestrator and specialists.\"\"\"\n",
    "    model_id = agent_data[\"model_config\"][\"model_id\"]\n",
    "\n",
    "    specialist_agents = []\n",
    "\n",
    "    temperature = agent_data[\"model_config\"][\"temperature\"]\n",
    "\n",
    "    primary_agent_id = agent_data[\"primary_agent_id\"]\n",
    "    agents_specs = agent_data[\"agents\"]\n",
    "    all_tool_adapters = environment.get_tools()  # Now returns Dict[str, Any]\n",
    "\n",
    "    # Build specialists first\n",
    "    specialist_agents = []\n",
    "    for agent_spec in agents_specs:\n",
    "        if agent_spec[\"agent_id\"] == primary_agent_id:\n",
    "            continue\n",
    "\n",
    "        seed = agent_spec.get(\"seed\")\n",
    "        model = get_model(model_id, temperature, seed)\n",
    "        spec_tool_adapters = filter_tool_adapters_by_prefix(all_tool_adapters, agent_spec[\"tools\"])\n",
    "        spec_tools = [adapter.tool for adapter in spec_tool_adapters.values()]\n",
    "        spec_tools.append(FinalAnswerTool())\n",
    "\n",
    "        specialist = ToolCallingAgent(\n",
    "            model=model,\n",
    "            tools=spec_tools,\n",
    "            name=sanitize_name(agent_spec[\"agent_name\"]),\n",
    "            description=agent_spec[\"agent_instruction\"],\n",
    "            instructions=agent_spec[\"agent_instruction\"],\n",
    "            verbosity_level=0,\n",
    "        )\n",
    "        specialist_agents.append(specialist)\n",
    "\n",
    "    # Build orchestrator\n",
    "    primary_spec = next(a for a in agents_specs if a[\"agent_id\"] == primary_agent_id)\n",
    "    primary_seed = primary_spec.get(\"seed\")\n",
    "    primary_model = get_model(model_id, temperature, primary_seed)\n",
    "\n",
    "    orchestrator = ToolCallingAgent(\n",
    "        model=primary_model,\n",
    "        tools=[FinalAnswerTool()],\n",
    "        managed_agents=specialist_agents if specialist_agents else None,\n",
    "        name=sanitize_name(primary_spec[\"agent_name\"]),\n",
    "        instructions=primary_spec[\"agent_instruction\"],\n",
    "        verbosity_level=0,\n",
    "    )\n",
    "\n",
    "    return [orchestrator], {agent.name: agent for agent in specialist_agents}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79568554",
   "metadata": {},
   "source": [
    "## Part 2: Organizing Evaluation with MASEval\n",
    "\n",
    "We've built a multi-agent system. Now let's see how **MASEval** helps us evaluate it systematically across multiple tasks.\n",
    "\n",
    "MASEval provides three key abstractions:\n",
    "\n",
    "1. **Task** - A single evaluation scenario (query + environment + evaluation criteria)\n",
    "2. **Environment** - Manages tools and state for a task\n",
    "3. **Benchmark** - Orchestrates running agents on tasks and collecting results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dc0505",
   "metadata": {},
   "source": [
    "### 2.1 The Environment Class\n",
    "\n",
    "An `Environment` creates and manages tools for each task. It also enables automatic tool tracing.\n",
    "\n",
    "**Key methods**:\n",
    "- `setup_state()`: Initialize tool state (email inboxes, bank accounts, etc.)\n",
    "- `create_tools()`: Create and convert tools to framework-specific format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d95d447",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiveADayEnvironment(Environment):\n",
    "    \"\"\"Environment that creates framework-specific tools from task data.\"\"\"\n",
    "\n",
    "    def __init__(self, task_data: Dict[str, Any], callbacks: List | None = None):\n",
    "        super().__init__(task_data, callbacks)\n",
    "\n",
    "    def setup_state(self, task_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Initialize environment state from task data.\"\"\"\n",
    "        env_data = task_data[\"environment_data\"].copy()\n",
    "        tool_names = env_data.get(\"tools\", [])\n",
    "\n",
    "        # Create state objects (e.g., email inboxes, bank accounts)\n",
    "        states = get_states(tool_names, env_data)\n",
    "        env_data.update(states)\n",
    "\n",
    "        return env_data\n",
    "\n",
    "    def create_tools(self) -> Dict[str, Any]:\n",
    "        \"\"\"Create and convert tools to framework-specific format, keyed by name.\"\"\"\n",
    "        tools_dict: Dict[str, Any] = {}\n",
    "\n",
    "        # Map tool names to their collection classes\n",
    "        tool_mapping = {\n",
    "            \"email\": (EmailToolCollection, lambda: (self.state[\"email_state\"],)),\n",
    "            \"banking\": (BankingToolCollection, lambda: (self.state[\"banking_state\"],)),\n",
    "            \"calculator\": (CalculatorToolCollection, lambda: ()),\n",
    "            \"python_executor\": (CodeExecutionToolCollection, lambda: (self.state[\"python_executor_state\"],)),\n",
    "            \"family_info\": (FamilyInfoToolCollection, lambda: (self.state[\"family_info\"],)),\n",
    "            \"stock_price\": (StockPriceToolCollection, lambda: (self.state[\"stock_price_lookup\"],)),\n",
    "            \"calendar\": (CalendarToolCollection, lambda: (self.state[\"calendar_state\"],)),\n",
    "            \"hotel_search\": (HotelSearchToolCollection, lambda: (self.state[\"hotel_search_state\"],)),\n",
    "            \"my_calendar_mcp\": (MCPCalendarToolCollection, lambda: (self.state[\"my_calendar_mcp_state\"],)),\n",
    "            \"other_calendar_mcp\": (MCPCalendarToolCollection, lambda: (self.state[\"other_calendar_mcp_state\"],)),\n",
    "        }\n",
    "\n",
    "        for tool_name in self.state[\"tools\"]:\n",
    "            if tool_name in tool_mapping:\n",
    "                ToolClass, get_init_args = tool_mapping[tool_name]\n",
    "                tool_instance = ToolClass(*get_init_args())\n",
    "\n",
    "                # Get base tools and convert to framework format\n",
    "                for base_tool in tool_instance.get_sub_tools():\n",
    "                    framework_tool = base_tool.to_smolagents()\n",
    "                    tool_key = getattr(base_tool, \"name\", None) or str(type(base_tool).__name__)\n",
    "                    tools_dict[tool_key] = framework_tool\n",
    "\n",
    "        return tools_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddd1429",
   "metadata": {},
   "source": [
    "### 2.2 Check Agents\n",
    "\n",
    "Let's verify our agent setup by building agents for the first task and inspecting their configuration.\n",
    "\n",
    "First, we check the agent config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d608504",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{config_0['task_description']}\")\n",
    "\n",
    "for i, agent_spec in enumerate(config_0[\"agents\"], 1):\n",
    "    print(f\"{i}. {agent_spec['agent_name']} (ID: {agent_spec['agent_id']})\")\n",
    "    print(f\"   Tools: {agent_spec['tools'] if agent_spec['tools'] else 'None (delegates only)'}\")\n",
    "    print(f\"   Role: {agent_spec['agent_instruction'][:80]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ba09aa",
   "metadata": {},
   "source": [
    "Now we implment the agents for the first task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbb228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the agents for task 0\n",
    "# Note: model_config is already set by load_benchmark_data()\n",
    "\n",
    "# Create environment from task data\n",
    "environment_0 = FiveADayEnvironment(\n",
    "    {\n",
    "        \"environment_data\": task_0.environment_data,\n",
    "        \"query\": task_0.query,\n",
    "        \"evaluation_data\": task_0.evaluation_data,\n",
    "        \"metadata\": task_0.metadata,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Build agents using the build_agents function\n",
    "agents_to_run, agents_to_monitor = build_agents(config_0, environment_0)\n",
    "\n",
    "print(f\"\\nBuilt Agents for Task: {task_0.metadata['task_id']}\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(f\"\\nAgents to run: {[agent.name for agent in agents_to_run]}\")\n",
    "print(f\"Agents to monitor: {list(agents_to_monitor.keys())}\")\n",
    "\n",
    "# Print details for each agent\n",
    "for agent in agents_to_run:\n",
    "    print(f\"\\n  Agent: {agent.name}\")\n",
    "    # smolagents stores tools as a dict with string keys\n",
    "    print(f\"    Tools: {list(agent.tools.keys())}\")\n",
    "    if hasattr(agent, \"managed_agents\") and agent.managed_agents:\n",
    "        # managed_agents is also a dict with string keys\n",
    "        print(f\"    Managed agents: {list(agent.managed_agents.keys())}\")\n",
    "        for agent_name, managed in agent.managed_agents.items():\n",
    "            print(f\"      - {managed.name}: {list(managed.tools.keys())}\")\n",
    "\n",
    "print(\"\\nAll agents built successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565199e8",
   "metadata": {},
   "source": [
    "### 2.3 The Benchmark Class\n",
    "\n",
    "A `Benchmark` orchestrates the entire evaluation process. It implements 5 key methods:\n",
    "\n",
    "1. **setup_environment()** - Create tools for a task\n",
    "2. **setup_agents()** - Build agents with appropriate tools\n",
    "3. **setup_evaluators()** - Create task-specific evaluators\n",
    "4. **run_agents()** - Execute agents and collect responses\n",
    "5. **evaluate()** - Run evaluators on agent outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c66cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiveADayBenchmark(Benchmark):\n",
    "    \"\"\"5-A-Day benchmark with multi-agent support.\"\"\"\n",
    "\n",
    "    def setup_environment(self, agent_data: Dict[str, Any], task: Task) -> Environment:\n",
    "        \"\"\"Create environment from task data.\"\"\"\n",
    "        task_data = {\n",
    "            \"environment_data\": task.environment_data,\n",
    "            \"query\": task.query,\n",
    "            \"evaluation_data\": task.evaluation_data,\n",
    "            \"metadata\": task.metadata,\n",
    "        }\n",
    "\n",
    "        environment = FiveADayEnvironment(task_data)\n",
    "\n",
    "        # Register all tools for tracing\n",
    "        for tool_name, tool_adapter in environment.get_tools().items():\n",
    "            self.register(\"tools\", tool_name, tool_adapter)\n",
    "\n",
    "        return environment\n",
    "\n",
    "    def setup_agents(\n",
    "        self, agent_data: Dict[str, Any], environment: Environment, task: Task, user=None\n",
    "    ) -> tuple[list[SmolAgentAdapter], Dict[str, SmolAgentAdapter]]:\n",
    "        \"\"\"Create multi-agent system with orchestrator and specialists.\"\"\"\n",
    "        agents_to_run, agents_to_monitor = build_agents(agent_data, environment)\n",
    "\n",
    "        # Create adapters for the primary agent(s) to run\n",
    "        adapters_to_run = [SmolAgentAdapter(agent, agent.name) for agent in agents_to_run]\n",
    "\n",
    "        # This ensures all agent traces are collected by the benchmark\n",
    "        all_agents = {agent.name: agent for agent in agents_to_run} | agents_to_monitor\n",
    "        adapters_to_monitor = {name: SmolAgentAdapter(agent, name) for name, agent in all_agents.items()}\n",
    "        return adapters_to_run, adapters_to_monitor\n",
    "\n",
    "    def setup_evaluators(self, environment, task, agents, user) -> Sequence[Evaluator]:\n",
    "        \"\"\"Create evaluators based on task's evaluation criteria.\"\"\"\n",
    "        if not task.evaluation_data[\"evaluators\"]:\n",
    "            return []\n",
    "\n",
    "        evaluator_instances = []\n",
    "        for name in task.evaluation_data[\"evaluators\"]:\n",
    "            evaluator_class = getattr(evaluators, name)\n",
    "            evaluator_instances.append(evaluator_class(task, environment, user))\n",
    "\n",
    "        return evaluator_instances\n",
    "\n",
    "    def run_agents(self, agents: Sequence[AgentAdapter], task: Task, environment: Environment, query: str) -> Sequence[Any]:\n",
    "        \"\"\"Execute agents and return their final answers.\"\"\"\n",
    "        answers = [agent.run(query) for agent in agents]\n",
    "        return answers\n",
    "\n",
    "    def get_model_adapter(self, model_id: str, **kwargs) -> ModelAdapter:\n",
    "        \"\"\"Return a model adapter for benchmark components that need LLM access.\n",
    "\n",
    "        This benchmark doesn't use simulated tools, user simulators, or LLM judges,\n",
    "        so this method is not called during execution.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"This benchmark doesn't use model adapters for tools/users/evaluators.\")\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        evaluators: Sequence[Evaluator],\n",
    "        agents: Dict[str, AgentAdapter],\n",
    "        final_answer: Any,\n",
    "        traces: Dict[str, Any],\n",
    "    ) -> list[Dict[str, Any]]:\n",
    "        \"\"\"Evaluate agent performance.\"\"\"\n",
    "        results = []\n",
    "        for evaluator in evaluators:\n",
    "            filtered_traces = evaluator.filter_traces(traces)\n",
    "            results.append(evaluator(filtered_traces, final_answer))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cd22cb",
   "metadata": {},
   "source": [
    "### 2.4 Loading All Tasks\n",
    "\n",
    "Now let's load all 5 tasks to run the full benchmark. We reuse `load_benchmark_data()` without specifying `task_indices` to get all tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04bbd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload all 5 tasks for the benchmark\n",
    "tasks, agent_configs = load_benchmark_data(\n",
    "    config_type=\"multi\",\n",
    "    framework=\"smolagents\",\n",
    "    model_id=\"gemini-2.5-flash\",\n",
    "    temperature=0.7,\n",
    "    seed=42,\n",
    "    # No task_indices = load all tasks\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(tasks)} tasks:\")\n",
    "for i, task in enumerate(tasks):\n",
    "    print(f\"  {i}. {task.metadata['task_id']}: {task.metadata['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1aa095f",
   "metadata": {},
   "source": [
    "### 2.5 Running the Benchmark\n",
    "\n",
    "Now we can run the complete benchmark! MASEval will:\n",
    "1. Create environments for each task\n",
    "2. Build multi-agent systems with appropriate tools\n",
    "3. Run agents and collect traces (tool calls, messages, etc.)\n",
    "4. Evaluate results using task-specific evaluators\n",
    "5. Log everything to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3764c0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run benchmark (will take approx. 2 min)\n",
    "benchmark = FiveADayBenchmark(\n",
    "    agent_data=agent_configs,\n",
    "    fail_on_setup_error=True,\n",
    "    fail_on_task_error=True,\n",
    "    fail_on_evaluation_error=True,\n",
    ")\n",
    "\n",
    "results = benchmark.run(tasks=tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fb97da",
   "metadata": {},
   "source": [
    "### 2.6 Examining Results\n",
    "\n",
    "Let's look at the results for two tasks to understand the output structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbd6f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "console = Console()\n",
    "\n",
    "for task in results[:2]:\n",
    "    task_id = task[\"task_id\"]\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Results for Task ID: {task_id}\")\n",
    "    print(\"=\" * 60)\n",
    "    traces = task[\"traces\"]\n",
    "    agent_traces = traces[\"agents\"]\n",
    "    print(f\"Traces available for agents: {list(agent_traces.keys())}\")\n",
    "    orchestrator_name = list(traces[\"agents\"].keys())[0]\n",
    "    print(f\"Last 5 messages for '{orchestrator_name}'\")\n",
    "    print(traces[\"agents\"].keys())\n",
    "    messages = traces[\"agents\"][orchestrator_name][\"messages\"]\n",
    "    for msg in messages[-5:]:\n",
    "        role = msg.get(\"role\", \"unknown\")\n",
    "        content = msg.get(\"content\", [])[0].get(\"text\", \"\")\n",
    "        panel = Panel.fit(\n",
    "            content,\n",
    "            title=f\" {role} \",\n",
    "            title_align=\"left\",\n",
    "        )\n",
    "        console.print(panel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecbe723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results for first two tasks\n",
    "for task in results[:2]:\n",
    "    task_id = task[\"task_id\"]\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Results for Task ID: {task_id}\")\n",
    "    print(\"=\" * 60)\n",
    "    eval_results = task[\"eval\"]\n",
    "    for evals in eval_results:\n",
    "        for k, v in evals.items():\n",
    "            print(f\"{k:<35} {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080f6216",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "You now understand how to build production agent benchmarks with MASEval:\n",
    "\n",
    "#### Part 1: Multi-Agent Systems\n",
    "- **Model creation** with LiteLLM for framework compatibility\n",
    "- **Framework-agnostic tools** that convert to any agent library\n",
    "- **Multi-agent architecture** with orchestrators and specialists\n",
    "- **Tool state management** for realistic task environments\n",
    "\n",
    "#### Part 2: MASEval Framework\n",
    "- **Task abstraction** packages queries, environments, and evaluation criteria\n",
    "- **Environment class** creates tools and enables automatic tracing\n",
    "- **Benchmark class** orchestrates evaluation across multiple tasks\n",
    "- **Custom evaluators** for diverse evaluation approaches (unit tests, LLM judges, etc.)\n",
    "- **Automatic tracing** captures all tool calls and agent interactions\n",
    "\n",
    "### Key Design Patterns\n",
    "\n",
    "1. **Separation of Concerns**:\n",
    "   - Tasks define WHAT to evaluate\n",
    "   - Environments provides a world in which the agents act (tools and state)\n",
    "   - Benchmarks orchestrate WHEN and WHERE\n",
    "   - Evaluators determine SUCCESS\n",
    "\n",
    "2. **Framework Agnostic**:\n",
    "   - Same tasks work with smolagents, LangGraph, LlamaIndex\n",
    "   - Tools convert automatically to framework-specific formats\n",
    "   - Easy to compare frameworks on identical tasks\n",
    "\n",
    "3. **Reproducibility**:\n",
    "   - Seeds derived systematically from task_id + agent_id\n",
    "   - All parameters logged automatically\n",
    "   - Results saved in structured JSONL format\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Explore evaluators** — Check `evaluators/` for different evaluation strategies\n",
    "2. **Try single-agent mode** — Load `data/singleagent.json` to compare architectures\n",
    "3. **Run from CLI** — Use `five_a_day_benchmark.py` for scripted runs with different frameworks\n",
    "4. **Add custom tasks** — Create your own task definitions and evaluators\n",
    "5. **Compare frameworks** — Run the same benchmark with LangGraph or LlamaIndex\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [MASEval Documentation](https://github.com/parameterlab/MASEval)\n",
    "- Example code: [`examples/five_a_day_benchmark/`](https://github.com/parameterlab/MASEval/tree/main/examples/five_a_day_benchmark)\n",
    "- Example data: [`examples/five_a_day_benchmark/data/`](https://github.com/parameterlab/MASEval/tree/main/examples/five_a_day_benchmark/data)\n",
    "- Tool implementations: [`examples/five_a_day_benchmark/tools/`](https://github.com/parameterlab/MASEval/tree/main/examples/five_a_day_benchmark/tools)\n",
    "- Evaluator implementations: [`examples/five_a_day_benchmark/evaluators/`](https://github.com/parameterlab/MASEval/tree/main/examples/five_a_day_benchmark/evaluators)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
